{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разведочный анализ данных и разметка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 0: Подготовка\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1: Импорт библиотек и настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Библиотеки загружены\n",
      "Текущая рабочая директория: /content\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import kagglehub\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "\n",
    "# Настройка отображения\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Библиотеки загружены\")\n",
    "print(\"Текущая рабочая директория:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2: Загрузка и первичный анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2806437580.py:7: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'contracts' dataset.\n",
      "First 5 records:                 regNum  \\\n",
      "0   166300031414000000   \n",
      "1   324300007314000000   \n",
      "2   366300032914000000   \n",
      "3  1010501417722000000   \n",
      "4  1010501746721000064   \n",
      "\n",
      "                                                                                                                                                                                       contractSubjectFull  \\\n",
      "0  1.1. Подрядчик обязуется выполнить работы по ремонту кровли и утеплению труб жилого дома №6, расположенного по адресу: Тульская область, Ленинский район, п. Молодежный, ул. Центральная (далее - об...   \n",
      "1  Исполнитель обязуется выполнить работу согласно Приложению № к Контракту и сдать ее результат Заказчику, а Заказчик обязуется принять результат работы и оплатить его. Качество работы Качество выпо...   \n",
      "2  1.1. По настоящему контракту поставщик обязуется поставить и передать заказчику в срок, предусмотренный настоящим контрактом, лекарственные препараты (далее - товар) в соответствии со спецификацие...   \n",
      "3  1.1. Поставщик обязуется осуществить поставку многофункциональных устройств (МФУ) (далее – Товар) согласно Спецификации (Приложение № 1 к настоящему контракту), а Заказчик в свою очередь обязуется...   \n",
      "4  ГОСУДАРСТВЕННЫЙ КОНТРАКТ № 08/23 Идентификационный код закупки- 211010501746701050100100300011723244 г. Майкоп\\t «14» сентября 2021 года Отдел Министерства внутренних дел Российской Федерации по г...   \n",
      "\n",
      "  OKPD2_codes  \n",
      "0        [43]  \n",
      "1        [71]  \n",
      "2        [21]  \n",
      "3        [26]  \n",
      "4        [17]  \n"
     ]
    }
   ],
   "source": [
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Указываем конкретный файл для загрузки\n",
    "file_path = \"contracts_dataset_filtered.json\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"aldarovalexander/contracts\",\n",
    "    file_path,\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200057, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Указываем конкретный файл для загрузки\n",
    "file_path = \"contracts_dataset_filtered.json\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"aldarovalexander/contracts\",\n",
    "    file_path,\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3: Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'contractSubject'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'contractSubject'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2133112634.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Объединение текстовых полей\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contractSubject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contractSubjectFull'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Длина текстов после объединения:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'contractSubject'"
     ]
    }
   ],
   "source": [
    "# Объединение текстовых полей\n",
    "df['full_text'] = df['contractSubject'].fillna('') + '. ' + df['contractSubjectFull'].fillna('')\n",
    "\n",
    "print(\"Длина текстов после объединения:\")\n",
    "print(df['full_text'].str.len().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовая очистка текста\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return \"\"\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удаление специальных символов\n",
    "    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\\s\\.]', ' ', text)\n",
    "    # Удаление лишних пробелов\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Удаление множественных точек\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    return text.strip()\n",
    "\n",
    "df['text_clean'] = df['full_text'].apply(clean_text)\n",
    "\n",
    "print(\"Пример очистки текста:\")\n",
    "print(\"ДО:\", df['full_text'].iloc[0][:200])\n",
    "print(\"ПОСЛЕ:\", df['text_clean'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка библиотек для продвинутой обработки (выполнить в терминале)\n",
    "# !pip install pymorphy2 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Скачивание стоп-слов\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "# Доменные стоп-слова\n",
    "domain_stopwords = ['контракт', 'договор', 'приложение', 'пункт', 'статья', \n",
    "                   'далее', 'согласно', 'также', 'например', 'иной', 'другой',\n",
    "                   'обязан', 'обязана', 'обязаны', 'обязано', 'условие', 'следующий']\n",
    "custom_stopwords = set(russian_stopwords + domain_stopwords)\n",
    "\n",
    "def advanced_text_processing(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Удаление стоп-слов\n",
    "    tokens = [token for token in tokens if token not in custom_stopwords]\n",
    "    \n",
    "    # Лемматизация\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            lemma = morph.parse(token)[0].normal_form\n",
    "            lemmatized_tokens.append(lemma)\n",
    "        except:\n",
    "            lemmatized_tokens.append(token)\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Применяем к данным\n",
    "print(\"Начата продвинутая обработка текста...\")\n",
    "df['text_processed'] = df['text_clean'].apply(advanced_text_processing)\n",
    "print(\"Обработка завершена!\")\n",
    "\n",
    "print(\"\\nПример продвинутой обработки:\")\n",
    "print(\"ДО:\", df['text_clean'].iloc[0][:150])\n",
    "print(\"ПОСЛЕ:\", df['text_processed'].iloc[0][:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 4: РАЗМЕТКА ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== АНАЛИЗ СУЩЕСТВУЮЩЕЙ РАЗМЕТКИ ===\")\n",
    "print(f\"Всего записей: {len(df)}\")\n",
    "print(f\"Записей с OKPD2: {df['OKPD2_codes'].notna().sum()}\")\n",
    "print(f\"Процент размеченных данных: {df['OKPD2_codes'].notna().mean():.2%}\")\n",
    "\n",
    "# Анализ структуры OKPD2 кодов\n",
    "df['okpd_count'] = df['OKPD2_codes'].apply(lambda x: len(x) if x else 0)\n",
    "print(\"\\nРаспределение количества кодов OKPD2 на запись:\")\n",
    "print(df['okpd_count'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечение и нормализация целевых меток\n",
    "def extract_okpd_label(okpd_codes):\n",
    "    \"\"\"Извлекает основной код OKPD2 для классификации\"\"\"\n",
    "    if not okpd_codes or len(okpd_codes) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Берем первый код из списка\n",
    "    primary_code = okpd_codes[0]\n",
    "    \n",
    "    # Определяем уровень детализации (начинаем с 2-значного кода)\n",
    "    if len(primary_code) >= 2:\n",
    "        return primary_code[:2]  # Первые 2 цифры - раздел\n",
    "    else:\n",
    "        return primary_code\n",
    "\n",
    "# Создаем целевую переменную\n",
    "df['target'] = df['OKPD2_codes'].apply(extract_okpd_label)\n",
    "\n",
    "print(\"Уровень детализации меток:\")\n",
    "print(df['target'].str.len().value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ качества разметки\n",
    "print(\"=== АНАЛИЗ КАЧЕСТВА РАЗМЕТКИ ===\")\n",
    "\n",
    "# Смотрим примеры текстов для разных классов\n",
    "sample_classes = df['target'].value_counts().head(3).index\n",
    "\n",
    "for class_label in sample_classes:\n",
    "    class_texts = df[df['target'] == class_label]['text_clean'].head(2)\n",
    "    print(f\"\\n--- Класс {class_label} ({len(df[df['target'] == class_label])} примеров) ---\")\n",
    "    for i, text in enumerate(class_texts):\n",
    "        print(f\"{i+1}. {text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обработка проблем разметки\n",
    "missing_target = df['target'].isna().sum()\n",
    "print(f\"Записей без меток: {missing_target}\")\n",
    "\n",
    "# Анализ малочисленных классов\n",
    "class_counts = df['target'].value_counts()\n",
    "small_classes = class_counts[class_counts < 5]\n",
    "print(f\"Малочисленные классы (менее 5 примеров): {len(small_classes)}\")\n",
    "\n",
    "# Решение: удаляем записи без меток и малочисленные классы\n",
    "df_labeled = df[df['target'].notna()].copy()\n",
    "df_labeled = df_labeled[~df_labeled['target'].isin(small_classes.index)]\n",
    "\n",
    "print(f\"Итоговый размер размеченного датасета: {len(df_labeled)}\")\n",
    "print(f\"Количество классов после фильтрации: {df_labeled['target'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Валидация разметки\n",
    "print(\"\\n=== ВАЛИДАЦИЯ РАЗМЕТКИ (случайные примеры) ===\")\n",
    "sample_indices = random.sample(range(len(df_labeled)), 5)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = df_labeled.iloc[idx]\n",
    "    print(f\"\\nМетка: {row['target']}\")\n",
    "    print(f\"Текст: {row['text_clean'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 5: Анализ размеченных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Статистика по классам\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Топ-20 классов\n",
    "top_classes = df_labeled['target'].value_counts().head(20)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=top_classes.values, y=top_classes.index)\n",
    "plt.title('Топ-20 самых частых классов ОКПД2')\n",
    "plt.xlabel('Количество примеров')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Распределение размеров классов\n",
    "class_sizes = df_labeled['target'].value_counts()\n",
    "sns.histplot(class_sizes, bins=30)\n",
    "plt.title('Распределение размеров классов')\n",
    "plt.xlabel('Примеров в классе')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Статистика по классам:\")\n",
    "print(f\"Всего классов: {len(class_sizes)}\")\n",
    "print(f\"Медианный размер класса: {class_sizes.median()}\")\n",
    "print(f\"Минимальный размер: {class_sizes.min()}\")\n",
    "print(f\"Максимальный размер: {class_sizes.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ длины текстов\n",
    "df_labeled['text_length'] = df_labeled['text_processed'].str.split().str.len()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=df_labeled, x='text_length', bins=50)\n",
    "plt.title('Общее распределение длины текстов')\n",
    "plt.xlabel('Длина текста (слов)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Длина текста по топ-10 классам\n",
    "top_10_classes = class_sizes.head(10).index\n",
    "df_top_classes = df_labeled[df_labeled['target'].isin(top_10_classes)]\n",
    "sns.boxplot(data=df_top_classes, x='target', y='text_length')\n",
    "plt.title('Длина текста по классам (топ-10)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Статистика длины текстов:\")\n",
    "print(df_labeled['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 6: Разделение на выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Стратифицированное разделение\n",
    "X = df_labeled['text_processed']\n",
    "y = df_labeled['target']\n",
    "\n",
    "# 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 ≈ 0.15\n",
    ")\n",
    "\n",
    "print(\"=== РАЗДЕЛЕНИЕ НА ВЫБОРКИ ===\")\n",
    "print(f\"Обучающая выборка: {len(X_train)} записей ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"Валидационная выборка: {len(X_val)} записей ({len(X_val)/len(X):.1%})\")\n",
    "print(f\"Тестовая выборка: {len(X_test)} записей ({len(X_test)/len(X):.1%})\")\n",
    "\n",
    "# Проверяем распределение классов в выборках\n",
    "print(\"\\nРаспределение классов по выборкам:\")\n",
    "for name, split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "    print(f\"{name}: {split.nunique()} классов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 7: Векторизация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# TF-IDF с разными настройками\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),  # Учитываем отдельные слова и пары\n",
    "    min_df=2,           # Игнорируем очень редкие слова\n",
    "    max_df=0.9,         # Игнорируем очень частые слова\n",
    "    stop_words=list(custom_stopwords)\n",
    ")\n",
    "\n",
    "# Bag-of-Words для сравнения\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    stop_words=list(custom_stopwords)\n",
    ")\n",
    "\n",
    "print(\"Векторизаторы созданы\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "print(\"Векторизация TF-IDF...\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Bag-of-Words\n",
    "print(\"Векторизация Bag-of-Words...\")\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_val_bow = bow_vectorizer.transform(X_val)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\nРазмерности матриц признаков:\")\n",
    "print(f\"TF-IDF: {X_train_tfidf.shape}\")\n",
    "print(f\"BOW: {X_train_bow.shape}\")\n",
    "\n",
    "# Сохранение processed данных\n",
    "df_processed = pd.DataFrame({\n",
    "    'text_processed': X,\n",
    "    'target': y\n",
    "})\n",
    "df_processed.to_csv('processed_contracts_data.csv', index=False)\n",
    "print(\"\\nОбработанные данные сохранены в 'processed_contracts_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Чекпойнт №3: Применение простых моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1: Определение метрик качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "\n",
    "# Для многоклассовой классификации с дисбалансом\n",
    "key_metric = 'f1_macro'  # F1-score (macro average)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Полная оценка модели\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    print(f\"=== {model_name} ===\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2: Baseline-модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "print(\"=== BASELINE МОДЕЛИ ===\")\n",
    "\n",
    "# KNN\n",
    "print(\"\\nОбучение KNN...\")\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_tfidf, y_train)\n",
    "y_pred_knn = knn.predict(X_val_tfidf)\n",
    "knn_metrics = evaluate_model(y_val, y_pred_knn, \"KNN\")\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\nОбучение Logistic Regression...\")\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial')\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = lr.predict(X_val_tfidf)\n",
    "lr_metrics = evaluate_model(y_val, y_pred_lr, \"Logistic Regression\")\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"\\nОбучение Naive Bayes...\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow, y_train)\n",
    "y_pred_nb = nb.predict(X_val_bow)\n",
    "nb_metrics = evaluate_model(y_val, y_pred_nb, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3: Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"=== ПОДБОР ГИПЕРПАРАМЕТРОВ ===\")\n",
    "\n",
    "# Оптимизация логистической регрессии\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'penalty': ['l2', 'none'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial'),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Запуск GridSearch...\")\n",
    "lr_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"Лучшие параметры: {lr_grid.best_params_}\")\n",
    "print(f\"Лучший F1-score на кросс-валидации: {lr_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 4: Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование лучшей модели\n",
    "best_model = lr_grid.best_estimator_\n",
    "y_pred_best = best_model.predict(X_val_tfidf)\n",
    "best_metrics = evaluate_model(y_val, y_pred_best, \"Best Logistic Regression\")\n",
    "\n",
    "# Сравнение всех моделей\n",
    "print(\"\\n=== СРАВНЕНИЕ МОДЕЛЕЙ ===\")\n",
    "models_comparison = pd.DataFrame({\n",
    "    'KNN': knn_metrics,\n",
    "    'LogisticRegression': lr_metrics,\n",
    "    'NaiveBayes': nb_metrics,\n",
    "    'BestModel': best_metrics\n",
    "})\n",
    "\n",
    "models_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация сравнения моделей\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
    "models_comparison.T[metrics_to_plot].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Сравнение метрик моделей')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Матрица ошибок лучшей модели\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(y_val, y_pred_best, labels=best_model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45, ax=plt.gca())\n",
    "plt.title('Матрица ошибок - Best Logistic Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ самых частых ошибок\n",
    "error_analysis = pd.DataFrame({\n",
    "    'true': y_val,\n",
    "    'predicted': y_pred_best,\n",
    "    'text': X_val\n",
    "})\n",
    "\n",
    "errors = error_analysis[y_val != y_pred_best]\n",
    "error_counts = errors.groupby(['true', 'predicted']).size().reset_index(name='count')\n",
    "error_counts = error_counts.sort_values('count', ascending=False)\n",
    "\n",
    "print(\"Самые частые ошибки классификации:\")\n",
    "print(error_counts.head(10))\n",
    "\n",
    "print(\"\\nПримеры ошибок:\")\n",
    "for i in range(min(3, len(errors))):\n",
    "    row = errors.iloc[i]\n",
    "    print(f\"\\nОшибка {i+1}:\")\n",
    "    print(f\"Истинный класс: {row['true']}\")\n",
    "    print(f\"Предсказанный класс: {row['predicted']}\")\n",
    "    print(f\"Текст: {row['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 5: Финальные выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ФИНАЛЬНЫЕ ВЫВОДЫ ===\")\n",
    "\n",
    "print(f\"1. РАЗМЕТКА ДАННЫХ:\")\n",
    "print(f\"   - Исходный датасет: {len(df)} записей\")\n",
    "print(f\"   - После очистки и разметки: {len(df_labeled)} записей\")\n",
    "print(f\"   - Количество классов: {df_labeled['target'].nunique()}\")\n",
    "print(f\"   - Сбалансированность: медианный размер класса {class_sizes.median()}\")\n",
    "\n",
    "print(f\"\\n2. КАЧЕСТВО МОДЕЛЕЙ (F1-macro):\")\n",
    "print(f\"   - KNN: {knn_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   - Naive Bayes: {nb_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   - Logistic Regression: {lr_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   - Best Model: {best_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. РЕКОМЕНДАЦИИ:\")\n",
    "print(f\"   - Лучшая модель: {type(best_model).__name__}\")\n",
    "print(f\"   - Ключевая метрика: {key_metric}\")\n",
    "print(f\"   - Основные проблемы: дисбаланс классов, семантически близкие классы\")\n",
    "\n",
    "# Сохранение лучшей модели\n",
    "import joblib\n",
    "\n",
    "model_artifacts = {\n",
    "    'model': best_model,\n",
    "    'vectorizer': tfidf_vectorizer,\n",
    "    'metrics': best_metrics\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifacts, 'best_model_artifacts.pkl')\n",
    "print(f\"\\nЛучшая модель сохранена в 'best_model_artifacts.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
