{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–µ–¥–º–µ—Ç–∞ –ø–æ –≤—Ç–æ—Ä–æ–º—É —É—Ä–æ–≤–Ω—é –û–ö–ü–î2\" ‚â° \"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –ø—Ä–µ–¥–º–µ—Ç–∞ —Å–¥–µ–ª–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞. –í–≤–µ–¥–µ–Ω–∏–µ.\n",
    "1. –ü–æ—Å–∫–æ–ª—å–∫—É –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–∏—Ç—å –±—ã—Å—Ç—Ä–æ –Ω–µ —É–¥–∞–ª–æ—Å—å, –±—ã–ª–æ –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—É–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –¥–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–∞ [–≥–æ—Å–∑–∞–∫—É–ø–∫–∞—Ö](https://zakupki.gov.ru/epz/main/public/home.html).\n",
    "2. –í —Å–≤—è–∑–∏ —Å —Ç–∞–∫–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–æ–¥—Ö–æ–¥–∞ –ø–æ—è–≤–∏–ª–∞—Å—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á–∏, —Ç–µ–ø–µ—Ä—å –æ–Ω–∞ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ —Ç–∞–∫: \"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–µ–¥–º–µ—Ç–∞ –ø–æ –≤—Ç–æ—Ä–æ–º—É —É—Ä–æ–≤–Ω—é –û–ö–ü–î2\". –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –û–ö–ü–î2 –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, [–∑–¥–µ—Å—å](https://www.consultant.ru/document/cons_doc_LAW_163703/). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Ü–µ–ª—å –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –¥–æ: –ø–æ —Ä–∞–∑–¥–µ–ª—É –ø—Ä–µ–¥–º–µ—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ –≤–µ—Ä–Ω—É—Ç—å –≤—Ç–æ—Ä–æ–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–¥–∞ –û–ö–ü–î2.\n",
    "–í–∞–∂–Ω–æ–µ –∑–∞–º–µ—á–∞–Ω–∏–µ: –ø–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –¥–æ–≥–æ–≤–æ—Ä–∞ –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ, —Ç–æ –º—ã –∏–º–µ–µ–º –¥–µ–ª–æ —Å –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π.\n",
    "3. –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 199913 –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç—Ä–∏ –ø–æ–ª—è \"regNum\" - —Ä–µ–µ—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞, \"contractSubjectFull\" - –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–µ–¥–º–µ—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞, \"OKPD2_codes\" - –Ω–∞–±–æ—Ä –û–ö–ü–î2 –∫–æ–¥–æ–≤ –∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞. –î–∞—Ç–∞—Å–µ—Ç —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏ –ø–æ [–∞–¥—Ä–µ—Å—É](https://www.kaggle.com/datasets/aldarovalexander/contract). –°–∞–º –¥–∞—Ç–∞—Å–µ—Ç –±—ã–ª –ø–æ–ª—É—á–µ–Ω –ø—É—Ç–µ–º –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–∞—Å—Ç–∏ –∏—Å–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –∑–∞ 2022 –≥–æ–¥, –∫–æ—Ç–æ—Ä—ã–µ –±—ã –∏–º–µ–ª–∏ —Ñ–æ—Ä–º–∞—Ç –≤–µ—Ä–Ω—ã–π DOCX –∏ —Ä–∞–∑–±–æ—Ä–∞ —Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —á–∞—Å—Ç–∏ –¥–æ–≥–æ–≤–æ—Ä–æ–≤. –ß–∞—Å—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–æ–ª—É—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ –≤–µ—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥–º–µ—Ç–∞ –≤ –¥–∞–Ω–Ω–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –æ–ø—É—â–µ–Ω–∞, –ø–æ—Å–∫–æ–ª—å–∫—É –¥–ª—è —Ü–µ–ª–µ–π —Å–∞–º–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–∞.\n",
    "4. –ë–ª–æ–∫–Ω–æ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –≤ [–∫–æ–ª–∞–±–µ](https://colab.research.google.com/github/aldarovav/YearProject/blob/main/analysis.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymorphy3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (2.0.6)\n",
      "Requirement already satisfied: dawg2-python>=0.8.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from pymorphy3) (0.9.0)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from pymorphy3) (2.4.417150.4580142)\n",
      "Requirement already satisfied: setuptools>=68.2.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from pymorphy3) (80.9.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy3\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import kagglehub\n",
    "import pymorphy3\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(\"–¢–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è:\", os.getcwd()) #–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—É—Ç–∞–Ω–∏—Ü—ã –º–µ–∂–¥—É —Å—Ä–µ–¥–∞–º–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–∞–∑–≤–µ–¥–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kagglehub import KaggleDatasetAdapter\n",
    "import pandas as pd\n",
    "\n",
    "# –£–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏\n",
    "file_path = \"contracts_dataset_unique.json\"\n",
    "\n",
    "# –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∏–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
    "dataset_path = kagglehub.dataset_download(\"aldarovalexander/contract\")\n",
    "full_file_path = f\"{dataset_path}/{file_path}\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∏–º JSON —Å —É–∫–∞–∑–∞–Ω–∏–µ–º dtype - —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –∏–Ω–∞—á–µ read_json –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç regNum –∫–∞–∫ —á–∏—Å–ª–æ –∏ –æ–±—Ä–µ–∑–∞–µ—Ç –µ–≥–æ, —á—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –Ω–µ—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏\n",
    "df = pd.read_json(full_file_path, dtype={'regNum': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# –ü–æ—Å–º–æ—Ç—Ä–∏–º –ø–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏\n",
    "print(\"–ü–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π:\", df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "print(f\"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
    "print(f\"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}\")\n",
    "print(\"\\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤: {df['regNum'].nunique()}\")\n",
    "print(f\"–ù–µ—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤: {df[df.duplicated('regNum', keep=False)]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—á–µ–Ω—å —Ö–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –≤—Å–µ –∑–∞–ø–∏—Å–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –û–ö–ü–î –∫–æ–¥–æ–≤ (—Å–ø–∏—Å–∫–æ–≤)\n",
    "all_okpd2_codes = df['OKPD2_codes'].explode() # –†–∞–∑–≤–µ—Ä–Ω–µ–º —Å–ø–∏—Å–∫–∏ –û–ö–ü–î2 –∫–æ–¥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "print(f\"–í—Å–µ–≥–æ –û–ö–ü–î2 –∫–æ–¥–æ–≤ (—Å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º–∏): {len(all_okpd2_codes)}\")\n",
    "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –û–ö–ü–î2 –∫–æ–¥–æ–≤: {all_okpd2_codes.nunique()}\")\n",
    "top_10_okpd2 = all_okpd2_codes.value_counts().head(10)\n",
    "print(f\"–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –û–ö–ü–î2 –∫–æ–¥–æ–≤: {top_10_okpd2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–æ–ø–∞–¥–∞–Ω–∏—è –∫–æ–¥–æ–≤ –û–ö–ü–î2 –≤ –¥–æ–≥–æ–≤–æ—Ä—ã\n",
    "okpd2_counts = df['OKPD2_codes'].str.len()\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ —Å –æ–¥–Ω–∏–º OKPD2 –∫–æ–¥–æ–º: {(okpd2_counts == 1).sum()}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ OKPD2 –∫–æ–¥–∞–º–∏: {(okpd2_counts > 1).sum()}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ OKPD2 –∫–æ–¥–æ–≤ –≤ –æ–¥–Ω–æ–º –∫–æ–Ω—Ç—Ä–∞–∫—Ç–µ: {okpd2_counts.max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤\n",
    "combinations = df['OKPD2_codes'].apply(tuple).value_counts()\n",
    "\n",
    "print(\"–¢–æ–ø-10 –Ω–∞–±–æ—Ä–æ–≤ –û–ö–ü–î2:\")\n",
    "print(combinations.head(10))\n",
    "\n",
    "print(f\"\\n–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {len(combinations)}\")\n",
    "print(f\"–ó–∞–ø–∏—Å–µ–π —Å –æ–¥–Ω–∏–º –∫–æ–¥–æ–º: {(df['OKPD2_codes'].apply(len) == 1).sum()}\")\n",
    "print(f\"–ó–∞–ø–∏—Å–µ–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∫–æ–¥–∞–º–∏: {(df['OKPD2_codes'].apply(len) > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –û–ö–ü–î2\n",
    "combinations = df['OKPD2_codes'].apply(tuple).value_counts()\n",
    "\n",
    "print(\"üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ù–ê–ë–û–†–û–í –û–ö–ü–î2\")\n",
    "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤: {len(combinations)}\")\n",
    "print(f\"–í—Å–µ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–æ–≤: {len(df)}\")\n",
    "\n",
    "# –¢–æ–ø-10 –Ω–∞–±–æ—Ä–æ–≤\n",
    "print(\"\\n–¢–æ–ø-10 –Ω–∞–±–æ—Ä–æ–≤:\")\n",
    "for i, (combo, count) in enumerate(combinations.head(10).items(), 1):\n",
    "    print(f\"{i}. {combo}: {count} –¥–æ–≥–æ–≤–æ—Ä–æ–≤\")\n",
    "\n",
    "# –ì—Ä–∞—Ñ–∏–∫\n",
    "\n",
    "combinations.plot(kind='line')\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –û–ö–ü–î2 –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–æ–≥–æ–≤–æ—Ä–æ–≤')\n",
    "plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–≥–æ–≤–æ—Ä–æ–≤')\n",
    "plt.yscale('log')  # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞ –ø–æ Y\n",
    "plt.xticks([])  # –£–±–∏—Ä–∞–µ–º –ø–æ–¥–ø–∏—Å–∏ –ø–æ –æ—Å–∏ X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏–¥–∏–º, —á—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–∞–∂–µ –Ω–∞ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π —à–∫–∞–ª–µ —Ç—Ä–µ–±—É–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º–∏–∑–∞—Ü–∏–∏, —á—Ç–æ  —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤, –ø–æ—Å—Ç—Ä–æ–∏–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –∫–∞–∫–∞—è —á–∞—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤ –∑–∞–º–µ—Ç–∞–µ—Ç –∫–∞–∫–æ–π –æ–±—ä—ë–º –¥–æ–≥–æ–≤–æ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
    "cumulative = combinations.cumsum()\n",
    "total = len(df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative) + 1), cumulative.values)\n",
    "\n",
    "# –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ—Å—å –∞–±—Å—Ü–∏—Å—Å –æ—Ç 1 –¥–æ 50\n",
    "plt.xlim(1, 50)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ª–∏–Ω–∏–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤\n",
    "percent_lines = [0.25, 0.5, 0.8, 0.9]\n",
    "for p in percent_lines:\n",
    "    value = total * p\n",
    "    plt.axhline(y=value, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.text(0, value, f'{p*100:.0f}%', va='bottom', ha='left')\n",
    "\n",
    "plt.title('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –û–ö–ü–î2')\n",
    "plt.ylabel('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–≥–æ–≤–æ—Ä–æ–≤')\n",
    "plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (–Ω–∞—Ä–∞—Å—Ç–∞—é—â–∏–º –∏—Ç–æ–≥–æ–º)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ—Ç —Ç–µ–ø–µ—Ä—å –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –≤–∏–¥–Ω–æ, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å 75% –ø–æ–ø–∞–¥–∞–Ω–∏–µ, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–µ—Ä–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–¥–∏–Ω –∏–∑ 20 –∫—Ä—É–ø–Ω–µ–π—à–∏—Ö –∫–ª–∞—Å—Å–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ contractSubjectFull: {df['contractSubjectFull'].str.len().mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ contractSubjectFull: {df['contractSubjectFull'].str.len().max()} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ contractSubjectFull: {df['contractSubjectFull'].str.len().min()} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "\n",
    "min_length = df['contractSubjectFull'].str.len().min()\n",
    "min_length_samples = df[df['contractSubjectFull'].str.len() == min_length].head(10)\n",
    "\n",
    "print(f\"\\n10 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω–æ–π ({min_length} —Å–∏–º–≤–æ–ª–æ–≤):\")\n",
    "for i, row in min_length_samples.iterrows():\n",
    "    print(f\"{i+1}. regNum: {row['regNum']}\")\n",
    "    print(f\"   contractSubjectFull: '{row['contractSubjectFull']}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—á–µ–≤–∏–¥–Ω–æ, —á—Ç–æ –µ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ–º –ø—Ä–µ–¥–º–µ—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞. –ï—Å—Ç—å –æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ –º–µ–Ω–µ–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ —è–≤–ª—è—é—Ç—Å—è –æ—à–∏–±–∫–∞–º–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã–±–æ—Ä–∫–∏, –ø—Ä–æ–≤–µ—Ä–∏–º —ç—Ç—É –≥–∏–ø–æ—Ç–µ–∑—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—ã–±–æ—Ä –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π —Å –¥–ª–∏–Ω–æ–π contractSubjectFull –º–µ–Ω–µ–µ 100 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "short_texts = df[df['contractSubjectFull'].str.len() < 100]\n",
    "\n",
    "print(f\"–ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π —Å –¥–ª–∏–Ω–æ–π —Ç–µ–∫—Å—Ç–∞ –º–µ–Ω–µ–µ 100 —Å–∏–º–≤–æ–ª–æ–≤: {len(short_texts)}\")\n",
    "print(f\"–≠—Ç–æ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {len(short_texts) / len(df) * 100:.1f}% –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π\n",
    "print(\"\\n–í—Å–µ –∑–∞–ø–∏—Å–∏ —Å –¥–ª–∏–Ω–æ–π contractSubjectFull < 100 —Å–∏–º–≤–æ–ª–æ–≤:\")\n",
    "for i, (index, row) in enumerate(short_texts.iterrows(), 1):\n",
    "    print(f\"{i}. –î–ª–∏–Ω–∞: {len(row['contractSubjectFull'])} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "    print(f\"   –¢–µ–∫—Å—Ç: '{row['contractSubjectFull']}'\")\n",
    "    \n",
    "    # –ï—Å–ª–∏ –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ —Å—Ç–æ–ª–±—Ü—ã, –≤—ã–≤–æ–¥–∏–º –∏—Ö —Ç–æ–∂–µ\n",
    "    other_columns = [col for col in df.columns if col != 'contractSubjectFull']\n",
    "    for col in other_columns:\n",
    "        print(f\"   {col}: {row[col]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏, —Å—á–∏—Ç–∞–µ–º –∏—Ö –æ—à–∏–±–æ—á–Ω—ã–º–∏ –∏ —É–¥–∞–ª–∏–º –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è\n",
    "indices_to_drop = df[df['contractSubjectFull'].str.len() < 100].index\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏\n",
    "df = df.drop(indices_to_drop)\n",
    "\n",
    "print(f\"–£–¥–∞–ª–µ–Ω–æ {len(indices_to_drop)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "print(f\"–û—Å—Ç–∞–ª–æ—Å—å {len(df)} –∑–∞–ø–∏—Å–µ–π\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–æ–≤ –û–ö–ü–î2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∏–º —á—Ç–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–≥–æ–≤–æ—Ä–∞ –≤–µ–∑–¥–µ –∑–∞–ø–æ–ª–Ω–µ–Ω –≤–µ—Ä–Ω–æ\n",
    "print(\"–î–ª–∏–Ω–∞ regNum:\")\n",
    "print(f\"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è: {df['regNum'].str.len().min()}\")\n",
    "print(f\"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {df['regNum'].str.len().max()}\")\n",
    "print(f\"  –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã: {sorted(df['regNum'].str.len().unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–¥–æ–≤ –Ω–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç\n",
    "\n",
    "count_distribution = okpd2_counts.value_counts().sort_index()\n",
    "for count, freq in count_distribution.items():\n",
    "    print(f\"  {count} –û–ö–ü–î2: {freq} –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ ({freq/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç\n",
    "all_okpd2_freq = all_okpd2_codes.value_counts()  # –≠—Ç–æ —Å–µ—Ç —Å —á–∞—Å—Ç–æ—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(all_okpd2_freq)), all_okpd2_freq.values, linewidth=1)\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –û–ö–ü–î2 –∫–æ–¥–æ–≤')\n",
    "plt.xlabel('–†–∞–Ω–≥ –û–ö–ü–î2 –∫–æ–¥–∞')\n",
    "plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
    "plt.grid(True)\n",
    "plt.yscale('log')  # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "print(f\"–ú–µ–¥–∏–∞–Ω–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {all_okpd2_freq.median()}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç–æ—Ç–∞: {all_okpd2_freq.mean():.2f}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {all_okpd2_freq.max()}\")\n",
    "print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {all_okpd2_freq.min()}\")\n",
    "print(f\"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {all_okpd2_freq.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –¥–æ–ª–µ–π\n",
    "total_mentions = all_okpd2_freq.sum()\n",
    "top_10_percent_share = all_okpd2_freq.head(int(len(all_okpd2_freq) * 0.1)).sum() / total_mentions * 100\n",
    "top_20_percent_share = all_okpd2_freq.head(int(len(all_okpd2_freq) * 0.2)).sum() / total_mentions * 100\n",
    "top_25_percent_share = all_okpd2_freq.head(int(len(all_okpd2_freq) * 0.25)).sum() / total_mentions * 100\n",
    "\n",
    "print(f\"–ù–∞ 10% —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–æ–¥–æ–≤ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è {top_10_percent_share:.1f}% –≤—Å–µ—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\")\n",
    "print(f\"–ù–∞ 20% —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–æ–¥–æ–≤ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è {top_20_percent_share:.1f}% –≤—Å–µ—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\")\n",
    "print(f\"–ù–∞ 25% —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–æ–¥–æ–≤ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è {top_25_percent_share:.1f}% –≤—Å–µ—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
    "cumulative_share = all_okpd2_freq.cumsum() / total_mentions * 100\n",
    "plt.plot(range(len(all_okpd2_freq)), cumulative_share, linewidth=2, color='red')\n",
    "plt.title('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ OKPD2 –∫–æ–¥–æ–≤')\n",
    "plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö OKPD2 –∫–æ–¥–æ–≤')\n",
    "plt.ylabel('–ù–∞–∫–æ–ø–ª–µ–Ω–Ω–∞—è –¥–æ–ª—è (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–µ–Ω—Ç–∏—Ä—ã\n",
    "plt.axhline(y=80, color='gray', linestyle='--', alpha=0.7, label='80% –≤—Å–µ—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π')\n",
    "plt.axvline(x=len(all_okpd2_freq) * 0.2, color='gray', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Ü–µ–ª–æ–º –∫–∞—Ä—Ç–∏–Ω–∞ –ø–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –∫–æ–¥–æ–≤ –∏ –∫–ª–∞—Å—Å–æ–≤ –ø–æ –∑–∞–º–µ—Ç–∞–µ–º—ã–º –¥–æ–ª—è–º —á–∏—Å–ª–∞ –≤—Å–µ—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –ø–æ—á—Ç–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç, —ç—Ç–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞–ª–æ –æ–∂–∏–¥–∞—Ç—å, –ø–æ—Å–∫–æ–ª—å–∫—É —Å–∞–º—ã–µ –∫—Ä—É–ø–Ω—ã–µ –∫–ª–∞—Å—Å—ã —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ –æ–¥–Ω–æ–º—É –∫–æ–¥—É –û–ö–ü–î2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "print(\"–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:\")\n",
    "print(df['contractSubjectFull'].str.len().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#–§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return \"\"\n",
    "\n",
    "    prev_text = text\n",
    "    while True:\n",
    "        text = text.lower()\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π, —Å—Å—ã–ª–æ–∫, —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π\n",
    "        text = re.sub(r'\\(?(–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ|–ø—Ä–∏–ª\\.?|—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏[—è–∏–µ–π]|–¥–∞–ª–µ–µ)\\s*‚Ññ?\\s*\\d*\\)?', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "        text = re.sub(r'(–∫–æ–Ω—Ç—Ä–∞–∫—Ç|–¥–æ–≥–æ–≤–æ—Ä)\\s*‚Ññ?\\s*\\d+[/\\-\\\\]\\d+', ' ', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π\\s+–∫–æ–¥\\s+–∑–∞–∫—É–ø–∫–∏[:\\-\\s]*[\\d\\w]+', ' ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –¥–∞—Ç\n",
    "        text = re.sub(r'\\d{1,2}\\s*[\\.\\/\\\\]\\s*\\d{1,2}\\s*[\\.\\/\\\\]\\s*\\d{2,4}', ' ', text)\n",
    "        text = re.sub(r'\"\\d{1,2}\"\\s*\\w+\\s*\\d{4}\\s*–≥–æ–¥–∞?', ' ', text)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–∞ ‚Ññ –∏ –Ω–æ–º–µ—Ä–æ–≤\n",
    "        text = re.sub(r'‚Ññ\\s*\\d*', ' ', text)\n",
    "        text = re.sub(r'\\s‚Ññ\\s|^‚Ññ\\s|\\s‚Ññ$', ' ', text)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω—É–º–µ—Ä–∞—Ü–∏–∏ –ø—É–Ω–∫—Ç–æ–≤ (1.1., 1.2. –∏ —Ç.–¥.)\n",
    "        text = re.sub(r'\\b\\d+\\.\\d+\\.', ' ', text)\n",
    "        text = re.sub(r'^\\d+\\.', ' ', text)  # –í –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Ü–∏—Ñ—Ä\n",
    "        text = re.sub(r'\\s\\d{1,2}\\s', ' ', text)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã)\n",
    "        text = re.sub(r'[^a-z–∞-—è0-9\\s]', ' ', text)\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        text = text.strip()\n",
    "        \n",
    "        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ—Å—Ç–∞–ª –º–µ–Ω—è—Ç—å—Å—è - –≤—ã—Ö–æ–¥–∏–º\n",
    "        if text == prev_text:\n",
    "            break\n",
    "        prev_text = text\n",
    "    \n",
    "    return text\n",
    "\n",
    "tqdm.pandas(desc=\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\")\n",
    "df['text_clean'] = df['contractSubjectFull'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 550\n",
    "print(\"–ü—Ä–∏–º–µ—Ä –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "print(df['regNum'].iloc[a])\n",
    "print(\"–î–û:\", df['contractSubjectFull'].iloc[a][:200])\n",
    "print(\"–ü–û–°–õ–ï:\", df['text_clean'].iloc[a][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "# –î–æ–º–µ–Ω–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "domain_stopwords = [\n",
    "    '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '–¥–æ–≥–æ–≤–æ—Ä', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–ø—É–Ω–∫—Ç', '—Å—Ç–∞—Ç—å—è', '–¥–∞–ª–µ–µ', \n",
    "    '—Å–æ–≥–ª–∞—Å–Ω–æ', '—Ç–∞–∫–∂–µ', '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–∏–Ω–æ–π', '–¥—Ä—É–≥–æ–π', '–æ–±—è–∑–∞–Ω', \n",
    "    '–æ–±—è–∑–∞–Ω–∞', '–æ–±—è–∑–∞–Ω—ã', '–æ–±—è–∑–∞–Ω–æ', '—É—Å–ª–æ–≤–∏–µ', '—Å–ª–µ–¥—É—é—â–∏–π', '–¥–æ–∫—É–º–µ–Ω—Ç', \n",
    "    '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '–ø—Ä–æ—Ç–æ–∫–æ–ª', '—Ä–µ—à–µ–Ω–∏–µ', '–∞–∫—Ç', '–æ—Ç—á–µ—Ç', '–Ω–∞—Å—Ç–æ—è—â–∏–π',\n",
    "    '–Ω–∞—Å—Ç–æ—è—â–µ–≥–æ', '–Ω–∞—Å—Ç–æ—è—â–µ–º', '–Ω–∞—Å—Ç–æ—è—â–µ–º—É']\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã\n",
    "prepositions = ['–ø–æ', '–≤', '–∏', '—Å', '—É', '–Ω–∞', '–∑–∞', '–∫', '–æ', '–æ—Ç', '–¥–æ', '–∏–∑', '–±–µ–∑']\n",
    "conjunctions = ['–∞', '–Ω–æ', '–∏–ª–∏', '—Ç–æ', '–∫–∞–∫', '—á—Ç–æ', '—á—Ç–æ–±—ã', 'if', 'and', 'or']\n",
    "\n",
    "custom_stopwords = set(russian_stopwords + domain_stopwords + prepositions + conjunctions)\n",
    "print(custom_stopwords)\n",
    "\n",
    "#–§—É–Ω–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in custom_stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä pymorphy3\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "#–§—É–Ω–∫—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            parsed = morph.parse(token)[0]\n",
    "            lemma = parsed.normal_form\n",
    "            lemmatized_tokens.append(lemma)\n",
    "        except Exception:\n",
    "            lemmatized_tokens.append(token)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ \n",
    "def tokenize_text(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "def advanced_text_processing(text):\n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–ø—É—Å—Ç–æ—Ç—É\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
    "    tokens = tokenize_text(cleaned_text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–ø–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    lemmatized_tokens = lemmatize_tokens(tokens)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –ø–æ—Å–ª–µ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (–≤—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥)\n",
    "    final_tokens = remove_stopwords(lemmatized_tokens)\n",
    "    \n",
    "    return ' '.join(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É\n",
    "print(\"–ù–∞—á–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
    "\n",
    "def sequential_text_processing(series):\n",
    "\n",
    "    texts = series.tolist()\n",
    "    total_texts = len(texts)\n",
    "   \n",
    "    results = []\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–µ–∫—Å—Ç —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º\n",
    "    for text in tqdm(texts, total=total_texts, desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\", unit=\"—Ç–µ–∫—Å—Ç\"):\n",
    "        result = advanced_text_processing(text)\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:\n",
    "print(\"=== –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ===\")\n",
    "df['text_processed'] = sequential_text_processing(df['contractSubjectFull'])\n",
    "\n",
    "print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 550\n",
    "print(\"–ü—Ä–∏–º–µ—Ä –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "print(df['regNum'].iloc[a])\n",
    "print(\"–î–û:\", df['contractSubjectFull'].iloc[a][:200])\n",
    "print(\"–ü–û–°–õ–ï:\", df['text_clean'].iloc[a][:200])\n",
    "print(\"–ü–û–°–õ–ï:\", df['text_processed'].iloc[a][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {df['contractSubjectFull'].str.len().mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {df['text_processed'].str.len().mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"–°–∂–∞—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞: {(1 - df['text_processed'].str.len().mean() / df['contractSubjectFull'].str.len().mean()) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "for i in range(min(5, len(df))):\n",
    "    print(f\"\\n–ü—Ä–∏–º–µ—Ä {i+1}:\")\n",
    "    original = df['contractSubjectFull'].iloc[i]\n",
    "    processed = df['text_processed'].iloc[i]\n",
    "    \n",
    "    print(\"–î–û:\", original[:150] + \"...\" if len(original) > 150 else original)\n",
    "    print(\"–ü–û–°–õ–ï:\", processed[:150] + \"...\" if len(processed) > 150 else processed)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "empty_processed = df['text_processed'].str.strip().eq('').sum()\n",
    "print(f\"–ü—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {empty_processed}\")\n",
    "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {df['text_processed'].nunique()}\")\n",
    "\n",
    "# –¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "from collections import Counter\n",
    "all_words = ' '.join(df['text_processed'].dropna()).split()\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"\\n–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
    "for word, freq in word_freq.most_common(10):\n",
    "    print(f\"  {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:\n",
    "  –æ–±—è–∑—É–µ—Ç—Å—è: 340701\n",
    "  ‚Ññ: 281243\n",
    "  (–¥–∞–ª–µ–µ: 235462\n",
    "  —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏: 215894\n",
    "  (–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: 205405\n",
    "  1: 202434\n",
    "  -: 198649\n",
    "  –¢–æ–≤–∞—Ä–∞: 190669\n",
    "  –ü–æ—Å—Ç–∞–≤—â–∏–∫: 183092\n",
    "  (–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: 178492\n",
    "\n",
    "–¢–∞–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –º–æ–∂–Ω–æ –ø–æ–¥—Ä–µ–∑–∞—Ç—å —Å–∫–æ–±–∫–∏, –∑–Ω–∞–∫–∏ –Ω–æ–º–µ—Ä–æ–≤, —Å–ª–æ–≤–∞: –æ–±—è–∑—É–µ—Ç—Å—è, –¥–∞–ª–µ–µ, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}\")\n",
    "print(f\"–ó–∞–ø–∏—Å–µ–π —Å OKPD2: {df['OKPD2_codes'].notna().sum()}\")\n",
    "print(f\"–ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {df['OKPD2_codes'].notna().mean():.2%}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã OKPD2 –∫–æ–¥–æ–≤\n",
    "df['okpd_count'] = df['OKPD2_codes'].apply(lambda x: len(x) if x else 0)\n",
    "print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–¥–æ–≤ OKPD2 –Ω–∞ –∑–∞–ø–∏—Å—å:\")\n",
    "print(df['okpd_count'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫\n",
    "def extract_okpd_labels(okpd_codes):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –∫–æ–¥—ã –û–ö–ü–î2 –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\"\"\"\n",
    "    if not okpd_codes or len(okpd_codes) == 0:\n",
    "        return []\n",
    "    \n",
    "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∫–æ–¥—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏—Ö –¥–æ 2-–∑–Ω–∞—á–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è\n",
    "    normalized_codes = []\n",
    "    for code in okpd_codes:\n",
    "        if len(code) >= 2:\n",
    "            normalized_codes.append(code[:2])  # –ü–µ—Ä–≤—ã–µ 2 —Ü–∏—Ñ—Ä—ã - —Ä–∞–∑–¥–µ–ª\n",
    "        else:\n",
    "            normalized_codes.append(code)\n",
    "    \n",
    "    return normalized_codes\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é - —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–æ–¥–æ–≤ –û–ö–ü–î2\n",
    "df['target'] = df['OKPD2_codes'].apply(extract_okpd_labels)\n",
    "\n",
    "print(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ç–æ–∫ –Ω–∞ –∑–∞–ø–∏—Å—å:\")\n",
    "print(df['target'].apply(len).value_counts().sort_index())\n",
    "\n",
    "print(\"\\n–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Ç–∫–∞–º:\")\n",
    "print(f\"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}\")\n",
    "print(f\"–ó–∞–ø–∏—Å–µ–π –±–µ–∑ –º–µ—Ç–æ–∫: {df['target'].apply(len).eq(0).sum()}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–∫\n",
    "all_labels = [label for sublist in df['target'] for label in sublist]\n",
    "print(f\"–í—Å–µ–≥–æ –º–µ—Ç–æ–∫ (—Å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º–∏): {len(all_labels)}\")\n",
    "\n",
    "print(\"\\n–£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—Ç–æ–∫:\")\n",
    "label_lengths = pd.Series([len(label) for label in all_labels])\n",
    "print(label_lengths.value_counts().sort_index())\n",
    "\n",
    "print(\"\\n–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –º–µ—Ç–æ–∫ –û–ö–ü–î2:\")\n",
    "from collections import Counter\n",
    "label_counts = Counter(all_labels)\n",
    "for label, count in label_counts.most_common(10):\n",
    "    print(f\"{label}: {count} —Ä–∞–∑\")\n",
    "\n",
    "print(f\"\\n–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫: {len(label_counts)}\")\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ (—Ç–µ–ø–µ—Ä—å —Å—Ç–æ–ª–±–µ—Ü 'target' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)\n",
    "all_labels = [label for sublist in df['target'] for label in sublist]\n",
    "unique_labels = sorted(set(all_labels))\n",
    "\n",
    "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {unique_labels}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É\n",
    "mlb = MultiLabelBinarizer(classes=unique_labels)\n",
    "target_matrix = mlb.fit_transform(df['target'])\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataFrame —Å –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏\n",
    "target_df = pd.DataFrame(target_matrix, columns=mlb.classes_, index=df.index)\n",
    "\n",
    "print(f\"\\n–°–æ–∑–¥–∞–Ω–æ {len(unique_labels)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –û–ö–ü–î2\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {target_matrix.shape}\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 5 –∑–∞–ø–∏—Å–µ–π:\")\n",
    "print(target_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "# –°–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–ª–∞—Å—Å–æ–≤\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ (–Ω–µ —Å–ø–∏—Å–∫–∏)\n",
    "top_labels = label_counts.most_common(5)\n",
    "sample_labels = [label for label, count in top_labels]\n",
    "\n",
    "for class_label in sample_labels:\n",
    "    # –ò—â–µ–º –∑–∞–ø–∏—Å–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –¥–∞–Ω–Ω—É—é –º–µ—Ç–∫—É\n",
    "    mask = df['target'].apply(lambda x: class_label in x)\n",
    "    class_texts = df[mask]['text_clean'].head(2)\n",
    "    \n",
    "    print(f\"\\n--- –ú–µ—Ç–∫–∞ {class_label} ({mask.sum()} –ø—Ä–∏–º–µ—Ä–æ–≤) ---\")\n",
    "    for i, text in enumerate(class_texts):\n",
    "        print(f\"{i+1}. {text[:150]}...\")\n",
    "    print(f\"–í—Å–µ –º–µ—Ç–∫–∏ –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {df[mask]['target'].iloc[0] if len(df[mask]) > 0 else '–Ω–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–±–ª–µ–º —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "\n",
    "# 1. –ê–Ω–∞–ª–∏–∑ –∑–∞–ø–∏—Å–µ–π –±–µ–∑ –º–µ—Ç–æ–∫ (–ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏)\n",
    "empty_targets = df['target'].apply(len) == 0\n",
    "print(f\"–ó–∞–ø–∏—Å–µ–π –±–µ–∑ –º–µ—Ç–æ–∫ (–ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏): {empty_targets.sum()}\")\n",
    "\n",
    "# 2. –ê–Ω–∞–ª–∏–∑ –º–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫\n",
    "# –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º\n",
    "all_labels = [label for sublist in df['target'] for label in sublist]\n",
    "label_counts = Counter(all_labels)\n",
    "\n",
    "# –ù–∞—Ö–æ–¥–∏–º –º–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (–º–µ–Ω–µ–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤)\n",
    "small_labels = {label: count for label, count in label_counts.items() if count < 5}\n",
    "print(f\"–ú–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ (–º–µ–Ω–µ–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤): {len(small_labels)}\")\n",
    "\n",
    "# 3. –ê–Ω–∞–ª–∏–∑ –∑–∞–ø–∏—Å–µ–π —Å —Ä–µ–¥–∫–∏–º–∏ –º–µ—Ç–∫–∞–º–∏\n",
    "records_with_small_labels = df['target'].apply(\n",
    "    lambda labels: any(label in small_labels for label in labels)\n",
    ").sum()\n",
    "print(f\"–ó–∞–ø–∏—Å–µ–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω—É—é –º–µ—Ç–∫—É: {records_with_small_labels}\")\n",
    "\n",
    "# 4. –†–µ—à–µ–Ω–∏–µ: –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π\n",
    "print(\"\\n=== –°–¢–†–ê–¢–ï–ì–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò ===\")\n",
    "\n",
    "# –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –±–µ–∑ –º–µ—Ç–æ–∫\n",
    "df_clean = df[~empty_targets].copy()\n",
    "print(f\"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö –º–µ—Ç–æ–∫: {len(df_clean)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "\n",
    "# –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –£–¥–∞–ª–µ–Ω–∏–µ –º–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –∏–∑ —Å–ø–∏—Å–∫–æ–≤\n",
    "def remove_small_labels(labels, small_labels_set):\n",
    "    \"\"\"–£–¥–∞–ª—è–µ—Ç –º–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–∞\"\"\"\n",
    "    return [label for label in labels if label not in small_labels_set]\n",
    "\n",
    "small_labels_set = set(small_labels.keys())\n",
    "df_clean['target_clean'] = df_clean['target'].apply(\n",
    "    lambda x: remove_small_labels(x, small_labels_set)\n",
    ")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Å—Ç–∞–ª–∏—Å—å –ª–∏ –∑–∞–ø–∏—Å–∏ –±–µ–∑ –º–µ—Ç–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\n",
    "empty_after_clean = df_clean['target_clean'].apply(len) == 0\n",
    "print(f\"–ó–∞–ø–∏—Å–µ–π –±–µ–∑ –º–µ—Ç–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {empty_after_clean.sum()}\")\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Å—Ç–∞–ª–∏—Å—å –±–µ–∑ –º–µ—Ç–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\n",
    "df_final = df_clean[~empty_after_clean].copy()\n",
    "\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df_final)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—á–∏—Å—Ç–∫–∏\n",
    "all_labels_final = [label for sublist in df_final['target_clean'] for label in sublist]\n",
    "label_counts_final = Counter(all_labels_final)\n",
    "\n",
    "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(label_counts_final)}\")\n",
    "print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –º–µ—Ç–∫—É: {min(label_counts_final.values())}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –º–µ—Ç–∫—É: {max(label_counts_final.values())}\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –º–µ—Ç–æ–∫ –Ω–∞ –∑–∞–ø–∏—Å—å –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\n",
    "labels_per_record_final = df_final['target_clean'].apply(len)\n",
    "print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ç–æ–∫ –Ω–∞ –∑–∞–ø–∏—Å—å –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:\")\n",
    "print(labels_per_record_final.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. –¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫\n",
    "plt.subplot(2, 2, 1)\n",
    "top_labels = label_counts_final.most_common(20)\n",
    "labels, counts = zip(*top_labels)\n",
    "sns.barplot(x=list(counts), y=list(labels))\n",
    "plt.title('–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –º–µ—Ç–æ–∫ –û–ö–ü–î2')\n",
    "plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π')\n",
    "\n",
    "# 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ç–æ–∫ –Ω–∞ –∑–∞–ø–∏—Å—å\n",
    "plt.subplot(2, 2, 2)\n",
    "labels_per_record = df['target'].apply(len)\n",
    "sns.histplot(labels_per_record, bins=30, discrete=True)\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ç–æ–∫ –Ω–∞ –∑–∞–ø–∏—Å—å')\n",
    "plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫')\n",
    "plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')\n",
    "\n",
    "# 3. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–æ–≤ (–æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫)\n",
    "plt.subplot(2, 2, 3)\n",
    "class_sizes = list(label_counts_final.values())\n",
    "sns.histplot(class_sizes, bins=30)\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫')\n",
    "plt.xlabel('–ß–∞—Å—Ç–æ—Ç–∞ –º–µ—Ç–∫–∏')\n",
    "plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫')\n",
    "\n",
    "# 4. –ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "plt.subplot(2, 2, 4)\n",
    "cumulative_counts = []\n",
    "current_sum = 0\n",
    "for count in sorted(class_sizes, reverse=True):\n",
    "    current_sum += count\n",
    "    cumulative_counts.append(current_sum)\n",
    "\n",
    "total_labels = sum(class_sizes)\n",
    "cumulative_percentage = [x / total_labels * 100 for x in cumulative_counts]\n",
    "\n",
    "plt.plot(range(1, len(cumulative_percentage) + 1), cumulative_percentage)\n",
    "plt.title('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–∫–∞–º–∏')\n",
    "plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø–æ–≤—ã—Ö –º–µ—Ç–æ–∫')\n",
    "plt.ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è –¥–∞–Ω–Ω—ã—Ö (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª –¥–∞–Ω–Ω—ã–º\n",
    "print(\"=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–£–õ–¨–¢–ò-–õ–ï–ô–ë–õ –î–ê–ù–ù–´–• ===\")\n",
    "print(f\"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}\")\n",
    "print(f\"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫: {len(label_counts_final)}\")\n",
    "print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π –º–µ—Ç–æ–∫: {sum(label_counts_final.values())}\")\n",
    "\n",
    "print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –º–µ—Ç–æ–∫ –Ω–∞ –∑–∞–ø–∏—Å—å:\")\n",
    "labels_count_stats = labels_per_record.value_counts().sort_index()\n",
    "for count, freq in labels_count_stats.items():\n",
    "    percentage = freq / len(df) * 100\n",
    "    print(f\"  {count} –º–µ—Ç–æ–∫: {freq} –∑–∞–ø–∏—Å–µ–π ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º:\")\n",
    "print(f\"–ú–µ–¥–∏–∞–Ω–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –º–µ—Ç–∫–∏: {pd.Series(class_sizes).median()}\")\n",
    "print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {min(class_sizes)}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {max(class_sizes)}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç–æ—Ç–∞: {pd.Series(class_sizes).mean():.1f}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è\n",
    "print(f\"\\n–ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –º–µ—Ç–æ–∫:\")\n",
    "top_n = [5, 10, 20, 50]\n",
    "for n in top_n:\n",
    "    if n <= len(label_counts_final):\n",
    "        coverage = sum([count for _, count in label_counts_final.most_common(n)]) / sum(label_counts_final.values()) * 100\n",
    "        print(f\"–¢–æ–ø-{n} –º–µ—Ç–æ–∫ –ø–æ–∫—Ä—ã–≤–∞—é—Ç {coverage:.1f}% –¥–∞–Ω–Ω—ã—Ö\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 1. –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –º–µ—Ç–æ–∫\n",
    "plt.subplot(1, 3, 1)\n",
    "class_sizes_series = pd.Series(class_sizes)\n",
    "sns.histplot(np.log1p(class_sizes_series), bins=30)\n",
    "plt.title('–õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –º–µ—Ç–æ–∫')\n",
    "plt.xlabel('log(—á–∞—Å—Ç–æ—Ç–∞ + 1)')\n",
    "plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫')\n",
    "\n",
    "# 2. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ vs –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π\n",
    "plt.subplot(1, 3, 2)\n",
    "labels_per_record.value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –º–µ—Ç–æ–∫')\n",
    "plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫')\n",
    "plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')\n",
    "\n",
    "# 3. –¢–æ–ø-30 –º–µ—Ç–æ–∫ (–±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ)\n",
    "plt.subplot(1, 3, 3)\n",
    "top_30_labels = label_counts_final.most_common(30)\n",
    "labels_30, counts_30 = zip(*top_30_labels)\n",
    "sns.barplot(x=list(counts_30), y=list(labels_30))\n",
    "plt.title('–¢–æ–ø-30 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –º–µ—Ç–æ–∫ –û–ö–ü–î2')\n",
    "plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –º–µ—Ç–æ–∫\n",
    "print(\"\\n=== –ê–ù–ê–õ–ò–ó –ö–û–ú–ë–ò–ù–ê–¶–ò–ô –ú–ï–¢–û–ö ===\")\n",
    "from collections import Counter\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –º–µ—Ç–æ–∫ (–ø–µ—Ä–≤—ã–µ 2-3 —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ)\n",
    "label_combinations = df['target'].apply(tuple).value_counts().head(10)\n",
    "\n",
    "print(\"–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –º–µ—Ç–æ–∫:\")\n",
    "for i, (combo, count) in enumerate(label_combinations.items(), 1):\n",
    "    print(f\"{i}. {list(combo)}: {count} –∑–∞–ø–∏—Å–µ–π ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏\n",
    "print(f\"\\n–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ (min/max): {min(class_sizes)/max(class_sizes):.4f}\")\n",
    "print(f\"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç: {pd.Series(class_sizes).std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ ML\n",
    "print(\"\\n=== –ê–ù–ê–õ–ò–ó –î–õ–Ø –ü–û–î–ì–û–¢–û–í–ö–ò –ö ML ===\")\n",
    "\n",
    "# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏\n",
    "imbalance_ratio = max(class_sizes) / min(class_sizes)\n",
    "print(f\"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: {imbalance_ratio:.1f}\")\n",
    "\n",
    "if imbalance_ratio > 100:\n",
    "    print(\"‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:\")\n",
    "    print(\"   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\")\n",
    "    print(\"   - –ü—Ä–∏–º–µ–Ω–∏—Ç—å oversampling/undersampling\")\n",
    "    print(\"   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏, —É—Å—Ç–æ–π—á–∏–≤—ã–µ –∫ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏\")\n",
    "elif imbalance_ratio > 10:\n",
    "    print(\"‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω–∞—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:\")\n",
    "    print(\"   - –í–∑–≤–µ—à–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\")\n",
    "    print(\"   - –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞\")\n",
    "else:\n",
    "    print(\"‚úì  –î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫\n",
    "avg_labels_per_record = labels_per_record.mean()\n",
    "print(f\"\\n–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ –Ω–∞ –∑–∞–ø–∏—Å—å: {avg_labels_per_record:.2f}\")\n",
    "\n",
    "if avg_labels_per_record > 3:\n",
    "    print(\"‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª–Ω–æ—Å—Ç—å - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:\")\n",
    "    print(\"   - Binary Relevance –ø–æ–¥—Ö–æ–¥\")\n",
    "    print(\"   - –ú–µ—Ç—Ä–∏–∫–∏: precision@k, coverage\")\n",
    "elif avg_labels_per_record > 1.5:\n",
    "    print(\"‚úì  –£–º–µ—Ä–µ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª–Ω–æ—Å—Ç—å - –ø–æ–¥—Ö–æ–¥—è—Ç:\")\n",
    "    print(\"   - Classifier Chains\")\n",
    "    print(\"   - Label Powerset\")\n",
    "else:\n",
    "    print(\"‚úì  –ù–∏–∑–∫–∞—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª–Ω–æ—Å—Ç—å - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\")\n",
    "    print(\"   - –õ—é–±—ã–µ –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª –º–µ—Ç–æ–¥—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "X = df['text_clean']  # –∏–ª–∏ 'text_processed', –µ—Å–ª–∏ –µ—Å—Ç—å\n",
    "y = df['target']  # —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏\n",
    "\n",
    "# –î–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º iterative_train_test_split –∏–∑ skmultilearn\n",
    "# –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º —Å–≤–æ—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "\n",
    "print(f\"–í—Å–µ–≥–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {len(X)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "\n",
    "# –°–ø–æ—Å–æ–± 1: –ü—Ä–æ—Å—Ç–æ–µ —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ (–±–µ–∑ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –∑–∞–ø–∏—Å–µ–π ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –∑–∞–ø–∏—Å–µ–π ({len(X_val)/len(X):.1%})\")\n",
    "print(f\"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –∑–∞–ø–∏—Å–µ–π ({len(X_test)/len(X):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[1])\n",
    "print(df['target'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# TF-IDF —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),  # –£—á–∏—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –ø–∞—Ä—ã\n",
    "    min_df=2,           # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞\n",
    "    max_df=0.9,         # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞\n",
    "    stop_words=list(custom_stopwords)\n",
    ")\n",
    "\n",
    "# Bag-of-Words –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    stop_words=list(custom_stopwords)\n",
    ")\n",
    "\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è TF-IDF...\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Bag-of-Words\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è Bag-of-Words...\")\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_val_bow = bow_vectorizer.transform(X_val)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\n–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–∞—Ç—Ä–∏—Ü –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
    "print(f\"TF-IDF: {X_train_tfidf.shape}\")\n",
    "print(f\"BOW: {X_train_bow.shape}\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ processed –¥–∞–Ω–Ω—ã—Ö\n",
    "df_processed = pd.DataFrame({\n",
    "    'text_processed': X,\n",
    "    'target': y\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (–¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞)\n",
    "dense_vector = X_train_tfidf[1].toarray()\n",
    "print(\"–ü–ª–æ—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Ñ–æ—Ä–º—ã:\", dense_vector.shape)\n",
    "print(\"–ù–µ–Ω—É–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã:\", np.count_nonzero(dense_vector))\n",
    "\n",
    "# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–µ–Ω—É–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Å–ª–æ–≤–∞–º–∏\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "nonzero_indices = X_train_tfidf[1].nonzero()[1]  # –∏–Ω–¥–µ–∫—Å—ã –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "print(\"\\n–°–ª–æ–≤–∞ –∏ –∏—Ö TF-IDF –≤–µ—Å–∞:\")\n",
    "for idx in nonzero_indices:\n",
    "    word = feature_names[idx]\n",
    "    weight = X_train_tfidf[1][0, idx]\n",
    "    print(f\"  '{word}': {weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ Bag-of-Words –º–∞—Ç—Ä–∏—Ü\n",
    "print(\"üîç –ê–ù–ê–õ–ò–ó BAG-OF-WORDS –ú–ê–¢–†–ò–¶\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "print(\"\\n1. –û–°–ù–û–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:\")\n",
    "print(f\"X_train_bow: {X_train_bow.shape} (–¥–æ–∫—É–º–µ–Ω—Ç—ã x —Å–ª–æ–≤–∞)\")\n",
    "print(f\"X_val_bow: {X_val_bow.shape}\")\n",
    "print(f\"X_test_bow: {X_test_bow.shape}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "def analyze_sparsity(matrix, name):\n",
    "    total = matrix.shape[0] * matrix.shape[1]\n",
    "    nonzero = matrix.nnz\n",
    "    density = (nonzero / total) * 100\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  –ù–µ–Ω—É–ª–µ–≤—ã—Ö: {nonzero:,} –∏–∑ {total:,}\")\n",
    "    print(f\"  –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {density:.4f}%\")\n",
    "    return density\n",
    "\n",
    "train_density = analyze_sparsity(X_train_bow, \"TRAIN\")\n",
    "analyze_sparsity(X_val_bow, \"VALIDATION\")\n",
    "analyze_sparsity(X_test_bow, \"TEST\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º\n",
    "print(f\"\\n2. –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ó–ù–ê–ß–ï–ù–ò–Ø–ú:\")\n",
    "print(f\"Min: {X_train_bow.min()}, Max: {X_train_bow.max()}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω–µ–µ: {X_train_bow.mean():.3f}\")\n",
    "print(f\"–ú–µ–¥–∏–∞–Ω–∞ –Ω–µ–Ω—É–ª–µ–≤—ã—Ö: {np.median(X_train_bow.data):.1f}\")\n",
    "\n",
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
    "train_nz_per_doc = X_train_bow.getnnz(axis=1)\n",
    "val_nz_per_doc = X_val_bow.getnnz(axis=1)\n",
    "\n",
    "print(f\"\\n3. –°–õ–û–í –í –î–û–ö–£–ú–ï–ù–¢–ê–•:\")\n",
    "print(f\"Train: min={train_nz_per_doc.min()}, max={train_nz_per_doc.max()}, avg={train_nz_per_doc.mean():.1f}\")\n",
    "print(f\"Val:   min={val_nz_per_doc.min()}, max={val_nz_per_doc.max()}, avg={val_nz_per_doc.mean():.1f}\")\n",
    "\n",
    "# –¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "word_freq = np.array(X_train_bow.sum(axis=0)).flatten()\n",
    "top_10_idx = np.argsort(word_freq)[::-1][:10]\n",
    "\n",
    "print(f\"\\n4. –¢–û–ü-10 –°–ê–ú–´–• –ß–ê–°–¢–´–• –°–õ–û–í:\")\n",
    "for i, idx in enumerate(top_10_idx, 1):\n",
    "    print(f\"{i:2}. {feature_names[idx]:15} {word_freq[idx]:,}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "print(f\"\\n5. –ü–ï–†–í–´–ô –î–û–ö–£–ú–ï–ù–¢:\")\n",
    "first_doc = X_train_bow[0]\n",
    "print(f\"–ù–µ–Ω—É–ª–µ–≤—ã—Ö —Å–ª–æ–≤: {first_doc.nnz}\")\n",
    "if first_doc.nnz > 0:\n",
    "    words = [feature_names[i] for i in first_doc.indices[:5]]\n",
    "    values = first_doc.data[:5]\n",
    "    print(f\"–ü–µ—Ä–≤—ã–µ 5 —Å–ª–æ–≤: {list(zip(words, values))}\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(X_train_bow.data, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('–ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞')\n",
    "plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# –¢–æ–ø-15 —Å–ª–æ–≤\n",
    "plt.subplot(1, 2, 2)\n",
    "top_15_idx = top_10_idx[:15] if len(top_10_idx) >= 15 else top_10_idx\n",
    "top_words = [feature_names[i] for i in top_15_idx]\n",
    "top_freqs = word_freq[top_15_idx]\n",
    "\n",
    "plt.barh(range(len(top_words)), top_freqs[::-1])\n",
    "plt.yticks(range(len(top_words)), top_words[::-1])\n",
    "plt.xlabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
    "plt.title('–¢–æ–ø-15 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"–í–´–í–û–î–´:\")\n",
    "print(f\"‚Ä¢ –ú–∞—Ç—Ä–∏—Ü—ã –æ—á–µ–Ω—å —Ä–∞–∑—Ä–µ–∂–µ–Ω—ã ({train_density:.4f}% –ø–ª–æ—Ç–Ω–æ—Å—Ç—å)\")\n",
    "print(f\"‚Ä¢ –í —Å—Ä–µ–¥–Ω–µ–º {train_nz_per_doc.mean():.1f} —Å–ª–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç\")\n",
    "print(f\"‚Ä¢ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(feature_names):,}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "def analyze_features(vectorizer, X_matrix, name):\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\"\"\"\n",
    "    print(f\"\\n=== –ê–ù–ê–õ–ò–ó {name} –ü–†–ò–ó–ù–ê–ö–û–í ===\")\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    feature_freq = np.array(X_matrix.sum(axis=0)).flatten()\n",
    "    \n",
    "    print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_freq)}\")\n",
    "    print(f\"–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã: {1 - (X_matrix.nnz / (X_matrix.shape[0] * X_matrix.shape[1])):.3%}\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç–æ—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞: {feature_freq.mean():.2f}\")\n",
    "    print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞: {feature_freq.max()}\")\n",
    "    \n",
    "    # –¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_indices = feature_freq.argsort()[-20:][::-1]\n",
    "    \n",
    "    print(f\"\\n–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        print(f\"{i:2d}. {feature_names[idx]}: {int(feature_freq[idx])}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –¥–ª—è TF-IDF\n",
    "analyze_features(tfidf_vectorizer, X_train_tfidf, \"TF-IDF\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –¥–ª—è BoW\n",
    "analyze_features(bow_vectorizer, X_train_bow, \"BAG-OF-WORDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ n-–≥—Ä–∞–º–º\n",
    "def analyze_ngrams(vectorizer, name):\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ uni-grams –∏ bi-grams\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    unigrams = [f for f in feature_names if ' ' not in f]\n",
    "    bigrams = [f for f in feature_names if ' ' in f]\n",
    "    \n",
    "    print(f\"Uni-grams: {len(unigrams)} ({len(unigrams)/len(feature_names)*100:.1f}%)\")\n",
    "    print(f\"Bi-grams: {len(bigrams)} ({len(bigrams)/len(feature_names)*100:.1f}%)\")\n",
    "    \n",
    "    # –ü—Ä–∏–º–µ—Ä—ã bi-grams\n",
    "    if bigrams:\n",
    "        print(f\"\\n–ü—Ä–∏–º–µ—Ä—ã bi-grams (–ø–µ—Ä–≤—ã–µ 10):\")\n",
    "        for bg in bigrams[:10]:\n",
    "            print(f\"  - {bg}\")\n",
    "\n",
    "analyze_ngrams(tfidf_vectorizer, \"TF-IDF\")\n",
    "analyze_ngrams(bow_vectorizer, \"BoW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç–æ–∫ (—Ç–æ–ª—å–∫–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)\n",
    "def get_top_features_for_labels(vectorizer, X_matrix, y, top_n=10):\n",
    "    \"\"\"–ù–∞—Ö–æ–¥–∏—Ç —Å–∞–º—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç–æ–∫\"\"\"\n",
    "    # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–∫–∏ (–ø–µ—Ä–≤—ã–µ –º–µ—Ç–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–æ–≤)\n",
    "    primary_labels = [labels[0] if labels else 'none' for labels in y]\n",
    "    \n",
    "    # –ë–µ—Ä–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –º–µ—Ç–æ–∫\n",
    "    top_labels = pd.Series(primary_labels).value_counts().head(5).index\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()   \n",
    " \n",
    "    for label in top_labels:\n",
    "        # –ò–Ω–¥–µ–∫—Å—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —ç—Ç–æ–π –º–µ—Ç–∫–æ–π\n",
    "        label_indices = [i for i, lbls in enumerate(y) if label in lbls]\n",
    "        \n",
    "        if not label_indices:\n",
    "            continue\n",
    "            \n",
    "        # –°—Ä–µ–¥–Ω–∏–π TF-IDF –¥–ª—è —ç—Ç–æ–π –º–µ—Ç–∫–∏\n",
    "        label_vector = X_matrix[label_indices].mean(axis=0)\n",
    "        label_scores = np.array(label_vector).flatten()\n",
    "        \n",
    "        # –¢–æ–ø-N –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        top_indices = label_scores.argsort()[-top_n:][::-1]\n",
    "        \n",
    "        print(f\"\\n–ú–µ—Ç–∫–∞ {label}:\")\n",
    "        for i, idx in enumerate(top_indices, 1):\n",
    "            if label_scores[idx] > 0:\n",
    "                print(f\"  {i:2d}. {feature_names[idx]}: {label_scores[idx]:.4f}\")\n",
    "\n",
    "# –¢–æ–ª—å–∫–æ –¥–ª—è TF-IDF (—Ç–∞–∫ –∫–∞–∫ —É BoW –Ω–µ—Ç –≤–µ—Å–æ–≤)\n",
    "get_top_features_for_labels(tfidf_vectorizer, X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# –ö–ª—é—á–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞\n",
    "key_metric = 'accuracy'\n",
    "\n",
    "def evaluate_model_multilabel(y_true, y_pred, model_name):\n",
    "    \"\"\"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\"\"\"\n",
    "    # Accuracy –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª (–¥–æ–ª—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)\n",
    "    accuracy = np.mean([set(true) == set(pred) for true, pred in zip(y_true, y_pred)])\n",
    "    \n",
    "    print(f\"=== {model_name} ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª\n",
    "def train_and_evaluate_models_multilabel(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"–û–±—É—á–µ–Ω–∏–µ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\"\"\"\n",
    "    \n",
    "    # –ú–æ–¥–µ–ª–∏ —Å OneVsRest –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª\n",
    "    knn = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=5))\n",
    "    lr = OneVsRestClassifier(LogisticRegression(random_state=42, max_iter=1000))\n",
    "    \n",
    "    # KNN\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    acc_knn = evaluate_model_multilabel(y_test, y_pred_knn, \"KNN\")\n",
    "    \n",
    "    # –õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    acc_lr = evaluate_model_multilabel(y_test, y_pred_lr, \"Logistic Regression\")\n",
    "    \n",
    "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "    print(\"\\n=== –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===\")\n",
    "    print(f\"KNN Accuracy: {acc_knn:.4f}\")\n",
    "    print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "    \n",
    "    if acc_knn > acc_lr:\n",
    "        print(\"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: KNN\")\n",
    "    elif acc_lr > acc_knn:\n",
    "        print(\"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: Logistic Regression\")\n",
    "    else:\n",
    "        print(\"–ú–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—É—é —Ç–æ—á–Ω–æ—Å—Ç—å\")\n",
    "    \n",
    "    return knn, lr, acc_knn, acc_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 2: Baseline-–º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "vectorizers_data = {\n",
    "    'TF-IDF': (X_train_tfidf, X_val_tfidf),\n",
    "    'BOW': (X_train_bow, X_val_bow)\n",
    "}\n",
    "\n",
    "print(\"üöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "all_results = {}\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
    "y_train_bin = mlb.fit_transform(y_train)\n",
    "y_val_bin = mlb.transform(y_val)\n",
    "\n",
    "# –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–æ–≤\n",
    "for vec_name, (X_train_vec, X_val_vec) in tqdm(vectorizers_data.items(), desc=\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã\"):\n",
    "    print(f\"\\nüìä {vec_name}:\")\n",
    "    \n",
    "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º\n",
    "    models = {\n",
    "        'KNN': OneVsRestClassifier(KNeighborsClassifier(n_neighbors=5)),\n",
    "        'LogisticRegression': OneVsRestClassifier(LogisticRegression(random_state=42, max_iter=1000))\n",
    "    }\n",
    "    \n",
    "    vec_results = {}\n",
    "    \n",
    "    for model_name, model in tqdm(models.items(), desc=\"–ú–æ–¥–µ–ª–∏\", leave=False):\n",
    "        # –û–±—É—á–µ–Ω–∏–µ\n",
    "        model.fit(X_train_vec, y_train_bin)\n",
    "        \n",
    "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞\n",
    "        y_pred_bin = model.predict(X_val_vec)\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º accuracy –¥–ª—è –º—É–ª—å—Ç–∏-–ª–µ–π–±–ª (–¥–æ–ª—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)\n",
    "        accuracy = accuracy_score(y_val_bin, y_pred_bin)\n",
    "        \n",
    "        vec_results[model_name] = {'model': model, 'accuracy': accuracy}\n",
    "        print(f\"   {model_name}: {accuracy:.4f}\")\n",
    "    \n",
    "    all_results[vec_name] = vec_results\n",
    "\n",
    "# –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "print(\"\\nüìà –ò–¢–û–ì–ò:\")\n",
    "results_list = []\n",
    "for vec_name, models in all_results.items():\n",
    "    for model_name, result in models.items():\n",
    "        results_list.append({\n",
    "            '–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä': vec_name,\n",
    "            '–ú–æ–¥–µ–ª—å': model_name,\n",
    "            'Accuracy': result['accuracy']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "def compare_models(results_dict):\n",
    "    \"\"\"–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    comparison_data = []\n",
    "    for model_name, result in results_dict.items():\n",
    "        metrics = result['metrics']\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'F1 Macro': metrics['f1_macro'],\n",
    "            'F1 Weighted': metrics['f1_weighted'],\n",
    "            'F1 Micro': metrics['f1_micro'],\n",
    "            'F1 Samples': metrics['f1_samples']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('F1 Macro', ascending=False)\n",
    "    \n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫\n",
    "def analyze_errors(y_true, y_pred, X_texts, top_n=10):\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for i, (true_labels, pred_labels, text) in enumerate(zip(y_true, y_pred, X_texts)):\n",
    "        true_set = set(true_labels)\n",
    "        pred_set = set(pred_labels)\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫\n",
    "        false_positives = pred_set - true_set\n",
    "        false_negatives = true_set - pred_set\n",
    "        \n",
    "        if false_positives or false_negatives:\n",
    "            errors.append({\n",
    "                'index': i,\n",
    "                'text_preview': text[:100] + '...',\n",
    "                'true_labels': true_labels,\n",
    "                'pred_labels': pred_labels,\n",
    "                'false_positives': list(false_positives),\n",
    "                'false_negatives': list(false_negatives),\n",
    "                'error_score': len(false_positives) + len(false_negatives)\n",
    "            })\n",
    "    \n",
    "    errors_df = pd.DataFrame(errors)\n",
    "    \n",
    "    if len(errors_df) > 0:\n",
    "        print(f\"\\n=== –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö (–¢–û–ü-{top_n}) ===\")\n",
    "        top_errors = errors_df.nlargest(top_n, 'error_score')\n",
    "        \n",
    "        for _, error in top_errors.iterrows():\n",
    "            print(f\"\\n–û—à–∏–±–∫–∞ #{error['index']} (—Å—á–µ—Ç: {error['error_score']}):\")\n",
    "            print(f\"–¢–µ–∫—Å—Ç: {error['text_preview']}\")\n",
    "            print(f\"–ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏: {error['true_labels']}\")\n",
    "            print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏: {error['pred_labels']}\")\n",
    "            if error['false_positives']:\n",
    "                print(f\"–õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è: {error['false_positives']}\")\n",
    "            if error['false_negatives']:\n",
    "                print(f\"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏: {error['false_negatives']}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    return errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_pipeline(X_train, X_val, X_test, y_train, y_val, y_test, vectorizer_name):\n",
    "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"–ó–ê–ü–£–°–ö –ü–ê–ô–ü–õ–ê–ô–ù–ê –î–õ–Ø {vectorizer_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏\n",
    "    results = train_baseline_models(X_train, y_train, X_val, y_val, vectorizer_name)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!\")\n",
    "        return None\n",
    "    \n",
    "    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏\n",
    "    comparison_df = compare_models(results)\n",
    "    \n",
    "    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    best_result = results[best_model_name]\n",
    "    \n",
    "    print(f\"\\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}\")\n",
    "    print(f\"üìä F1 Macro: {best_result['metrics']['f1_macro']:.4f}\")\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "    best_predictions = best_result['predictions']\n",
    "    errors_df = analyze_errors(y_val, best_predictions, X_val, top_n=8)\n",
    "    \n",
    "    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "    print(f\"\\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï\")\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    y_test_bin = best_result['mlb'].transform(y_test)\n",
    "    y_test_pred_bin = best_result['model'].predict(X_test)\n",
    "    y_test_pred = best_result['mlb'].inverse_transform(y_test_pred_bin)\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "    test_metrics = evaluate_multilabel_model(y_test, y_test_pred, f\"{best_model_name} - TEST SET\")\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_result['model'],\n",
    "        'best_model_name': best_model_name,\n",
    "        'mlb': best_result['mlb'],\n",
    "        'val_metrics': best_result['metrics'],\n",
    "        'test_metrics': test_metrics,\n",
    "        'all_results': results,\n",
    "        'comparison': comparison_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 3: –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"=== –ü–û–î–ë–û–† –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í ===\")\n",
    "\n",
    "# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'penalty': ['l2', 'none'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial'),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"–ó–∞–ø—É—Å–∫ GridSearch...\")\n",
    "lr_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {lr_grid.best_params_}\")\n",
    "print(f\"–õ—É—á—à–∏–π F1-score –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {lr_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 4: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "best_model = lr_grid.best_estimator_\n",
    "y_pred_best = best_model.predict(X_val_tfidf)\n",
    "best_metrics = evaluate_model(y_val, y_pred_best, \"Best Logistic Regression\")\n",
    "\n",
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π\n",
    "print(\"\\n=== –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===\")\n",
    "models_comparison = pd.DataFrame({\n",
    "    'KNN': knn_metrics,\n",
    "    'LogisticRegression': lr_metrics,\n",
    "    'NaiveBayes': nb_metrics,\n",
    "    'BestModel': best_metrics\n",
    "})\n",
    "\n",
    "models_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
    "models_comparison.T[metrics_to_plot].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(y_val, y_pred_best, labels=best_model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45, ax=plt.gca())\n",
    "plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - Best Logistic Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫\n",
    "error_analysis = pd.DataFrame({\n",
    "    'true': y_val,\n",
    "    'predicted': y_pred_best,\n",
    "    'text': X_val\n",
    "})\n",
    "\n",
    "errors = error_analysis[y_val != y_pred_best]\n",
    "error_counts = errors.groupby(['true', 'predicted']).size().reset_index(name='count')\n",
    "error_counts = error_counts.sort_values('count', ascending=False)\n",
    "\n",
    "print(\"–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\")\n",
    "print(error_counts.head(10))\n",
    "\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫:\")\n",
    "for i in range(min(3, len(errors))):\n",
    "    row = errors.iloc[i]\n",
    "    print(f\"\\n–û—à–∏–±–∫–∞ {i+1}:\")\n",
    "    print(f\"–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {row['true']}\")\n",
    "    print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {row['predicted']}\")\n",
    "    print(f\"–¢–µ–∫—Å—Ç: {row['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 5: –§–∏–Ω–∞–ª—å–Ω—ã–µ –≤—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== –§–ò–ù–ê–õ–¨–ù–´–ï –í–´–í–û–î–´ ===\")\n",
    "\n",
    "print(f\"1. –†–ê–ó–ú–ï–¢–ö–ê –î–ê–ù–ù–´–•:\")\n",
    "print(f\"   - –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(df)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "print(f\"   - –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –∏ —Ä–∞–∑–º–µ—Ç–∫–∏: {len(df_labeled)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "print(f\"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {df_labeled['target'].nunique()}\")\n",
    "print(f\"   - –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: –º–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Å–∞ {class_sizes.median()}\")\n",
    "\n",
    "print(f\"\\n2. –ö–ê–ß–ï–°–¢–í–û –ú–û–î–ï–õ–ï–ô (F1-macro):\")\n",
    "print(f\"   - KNN: {knn_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   - Naive Bayes: {nb_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   - Logistic Regression: {lr_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   - Best Model: {best_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\")\n",
    "print(f\"   - –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {type(best_model).__name__}\")\n",
    "print(f\"   - –ö–ª—é—á–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞: {key_metric}\")\n",
    "print(f\"   - –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ –∫–ª–∞—Å—Å—ã\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "import joblib\n",
    "\n",
    "model_artifacts = {\n",
    "    'model': best_model,\n",
    "    'vectorizer': tfidf_vectorizer,\n",
    "    'metrics': best_metrics\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifacts, 'best_model_artifacts.pkl')\n",
    "print(f\"\\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'best_model_artifacts.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
