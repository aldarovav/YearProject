{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–∞–∑–≤–µ–¥–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–∑–º–µ—Ç–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 0: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞\n",
    "1. –ü–æ—Å–∫–æ–ª—å–∫—É –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–∏—Ç—å –±—ã—Å—Ç—Ä–æ –Ω–µ —É–¥–∞–ª–æ—Å—å, –±—ã–ª–æ –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—É–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –¥–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–∞ [–≥–æ—Å–∑–∞–∫—É–ø–∫–∞—Ö](https://zakupki.gov.ru/epz/main/public/home.html).\n",
    "2. –í —Å–≤—è–∑–∏ —Å —Ç–∞–∫–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–æ–¥—Ö–æ–¥–∞ –ø–æ—è–≤–∏–ª–∞—Å—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á–∏, —Ç–µ–ø–µ—Ä—å –æ–Ω–∞ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ —Ç–∞–∫: \"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–µ–¥–º–µ—Ç–∞ –ø–æ –≤—Ç–æ—Ä–æ–º—É —É—Ä–æ–≤–Ω—é –û–ö–ü–î2\". –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –û–ö–ü–î2 –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, [–∑–¥–µ—Å—å](https://www.consultant.ru/document/cons_doc_LAW_163703/). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Ü–µ–ª—å –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –¥–æ: –ø–æ —Ä–∞–∑–¥–µ–ª—É –ø—Ä–µ–¥–º–µ—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ –≤–µ—Ä–Ω—É—Ç—å –≤—Ç–æ—Ä–æ–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–¥–∞ –û–ö–ü–î2.\n",
    "–í–∞–∂–Ω–æ–µ –∑–∞–º–µ—á–∞–Ω–∏–µ: –ø–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –¥–æ–≥–æ–≤–æ—Ä–∞ –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ, —Ç–æ –º—ã –∏–º–µ–µ–º –¥–µ–ª–æ —Å –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π.\n",
    "3. –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 200057 –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç—Ä–∏ –ø–æ–ª—è \"regNum\" - —Ä–µ–µ—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞, \"contractSubjectFull\" - –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–µ–¥–º–µ—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞, \"OKPD2_codes\" - –Ω–∞–±–æ—Ä –û–ö–ü–î2 –∫–æ–¥–æ–≤ –∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞. –î–∞—Ç–∞—Å–µ—Ç —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏ –ø–æ [–∞–¥—Ä–µ—Å—É](https://www.kaggle.com/datasets/aldarovalexander/contracts/data). –°–∞–º –¥–∞—Ç–∞—Å–µ—Ç –±—ã–ª –ø–æ–ª—É—á–µ–Ω –ø—É—Ç–µ–º –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–∞—Å—Ç–∏ –∏—Å–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –∑–∞ 2022 –≥–æ–¥, –∫–æ—Ç–æ—Ä—ã–µ –±—ã –∏–º–µ–ª–∏ —Ñ–æ—Ä–º–∞—Ç –≤–µ—Ä–Ω—ã–π DOCX –∏ —Ä–∞–∑–±–æ—Ä–∞ —Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —á–∞—Å—Ç–∏ –¥–æ–≥–æ–≤–æ—Ä–æ–≤. –ß–∞—Å—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–æ–ª—É—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ –≤–µ—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥–º–µ—Ç–∞ –≤ –¥–∞–Ω–Ω–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –æ–ø—É—â–µ–Ω–∞, –ø–æ—Å–∫–æ–ª—å–∫—É –¥–ª—è —Ü–µ–ª–µ–π —Å–∞–º–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–∞.\n",
    "4. –ë–ª–æ–∫–Ω–æ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –≤ [–∫–æ–ª–∞–±–µ](https://colab.research.google.com/github/aldarovav/YearProject/blob/main/analysis.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 1: –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "–¢–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: /content\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import kagglehub\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(\"–¢–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –ü–†–û–í–ï–†–ö–ê –ü–û–î–ö–õ–Æ–ß–ï–ù–ù–´–• –î–ê–¢–ê–°–ï–¢–û–í ===\n",
      "üìÅ –ù–∞–π–¥–µ–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ /kaggle/input: 1\n",
      "  üìÇ contracts: ['contracts_dataset_filtered.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"=== –ü–†–û–í–ï–†–ö–ê –ü–û–î–ö–õ–Æ–ß–ï–ù–ù–´–• –î–ê–¢–ê–°–ï–¢–û–í ===\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é input\n",
    "input_dir = '/kaggle/input'\n",
    "if os.path.exists(input_dir):\n",
    "    datasets = os.listdir(input_dir)\n",
    "    print(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ /kaggle/input: {len(datasets)}\")\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dataset_path = os.path.join(input_dir, dataset)\n",
    "        files = os.listdir(dataset_path) if os.path.isdir(dataset_path) else []\n",
    "        print(f\"  üìÇ {dataset}: {files}\")\n",
    "else:\n",
    "    print(\"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è /kaggle/input –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /kaggle/input/contracts/contracts_dataset_unique.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2923730886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/contracts/contracts_dataset_unique.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'regNum'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞–ø—Ä—è–º—É—é!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mconvert_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     json_reader = JsonReader(\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ujson\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         ):\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {filepath_or_buffer} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             warnings.warn(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File /kaggle/input/contracts/contracts_dataset_unique.json does not exist"
     ]
    }
   ],
   "source": [
    "# –ü—Ä—è–º–æ–µ —á—Ç–µ–Ω–∏–µ –∏–∑ –ø–æ–¥–∫–ª—é—á–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "file_path = '/kaggle/input/contracts/contracts_dataset_unique.json'\n",
    "\n",
    "df = pd.read_json(file_path, dtype={'regNum': str})\n",
    "\n",
    "print(\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞–ø—Ä—è–º—É—é!\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä: {df.shape}\")\n",
    "print(f\"–¢–∏–ø regNum: {df['regNum'].dtype}\")\n",
    "\n",
    "df.head(3)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ dataset_load()\n",
    "\n",
    "print(\"=== –ü–ï–†–í–ò–ß–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• ===\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
    "print(f\"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}\")\n",
    "print(\"\\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º regNum –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ç–∏–ø\n",
    "df['regNum'] = df['regNum'].astype(str)\n",
    "\n",
    "# –ë–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "print(\"\\n=== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó ===\")\n",
    "print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤: {df['regNum'].nunique()}\")\n",
    "print(f\"–ù–µ—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤: {df[df.duplicated('regNum', keep=False)]}\")\n",
    "\n",
    "# –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ OKPD2 –∫–æ–¥–æ–≤ (—Å–ø–∏—Å–∫–æ–≤)\n",
    "print(\"\\n=== –ê–ù–ê–õ–ò–ó OKPD2 –ö–û–î–û–í ===\")\n",
    "# –†–∞–∑–≤–µ—Ä–Ω–µ–º —Å–ø–∏—Å–∫–∏ OKPD2 –∫–æ–¥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "all_okpd2_codes = df['OKPD2_codes'].explode()\n",
    "print(f\"–í—Å–µ–≥–æ OKPD2 –∫–æ–¥–æ–≤ (—Å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º–∏): {len(all_okpd2_codes)}\")\n",
    "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö OKPD2 –∫–æ–¥–æ–≤: {all_okpd2_codes.nunique()}\")\n",
    "print(\"\\n–¢–æ–ø-10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö OKPD2 –∫–æ–¥–æ–≤:\")\n",
    "print(all_okpd2_codes.value_counts().head(10))\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–ø–∏—Å–∫–æ–≤ OKPD2\n",
    "print(\"\\n=== –°–¢–†–£–ö–¢–£–†–ê OKPD2 –ö–û–î–û–í ===\")\n",
    "okpd2_counts = df['OKPD2_codes'].str.len()\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ —Å –æ–¥–Ω–∏–º OKPD2 –∫–æ–¥–æ–º: {(okpd2_counts == 1).sum()}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ OKPD2 –∫–æ–¥–∞–º–∏: {(okpd2_counts > 1).sum()}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ OKPD2 –∫–æ–¥–æ–≤ –≤ –æ–¥–Ω–æ–º –∫–æ–Ω—Ç—Ä–∞–∫—Ç–µ: {okpd2_counts.max()}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "print(\"\\n=== –ü–†–û–ü–£–©–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø ===\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"\\n=== –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–û–í–´–• –î–ê–ù–ù–´–• ===\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ contractSubjectFull: {df['contractSubjectFull'].str.len().mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ contractSubjectFull: {df['contractSubjectFull'].str.len().max()} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ contractSubjectFull: {df['contractSubjectFull'].str.len().min()} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"\\n=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ö–û–õ–û–ù–ö–ê–ú ===\")\n",
    "print(\"–î–ª–∏–Ω–∞ regNum:\")\n",
    "print(f\"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è: {df['regNum'].str.len().min()}\")\n",
    "print(f\"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {df['regNum'].str.len().max()}\")\n",
    "print(f\"  –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã: {sorted(df['regNum'].str.len().unique())}\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è\n",
    "print(\"\\n=== –ü–†–ò–ú–ï–†–´ –î–ê–ù–ù–´–• ===\")\n",
    "print(\"–ü–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n–ó–∞–ø–∏—Å—å {i+1}:\")\n",
    "    print(f\"  regNum: {df.iloc[i]['regNum']}\")\n",
    "    print(f\"  OKPD2_codes: {df.iloc[i]['OKPD2_codes']}\")\n",
    "    print(f\"  contractSubjectFull (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {df.iloc[i]['contractSubjectFull'][:200]}...\")\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ OKPD2 –∫–æ–¥–æ–≤\n",
    "print(\"\\n=== –ê–ù–ê–õ–ò–ó –ö–û–ú–ë–ò–ù–ê–¶–ò–ô OKPD2 –ö–û–î–û–í ===\")\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–∫–∏ –≤ –∫–æ—Ä—Ç–µ–∂–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π\n",
    "okpd2_combinations = df['OKPD2_codes'].apply(tuple)\n",
    "print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π OKPD2 –∫–æ–¥–æ–≤: {okpd2_combinations.nunique()}\")\n",
    "print(\"–¢–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π OKPD2 –∫–æ–¥–æ–≤:\")\n",
    "top_combinations = okpd2_combinations.value_counts().head(5)\n",
    "for combo, count in top_combinations.items():\n",
    "    print(f\"  {list(combo)}: {count} –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–¥–æ–≤ –Ω–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç\n",
    "print(\"\\n=== –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–û–õ–ò–ß–ï–°–¢–í–ê OKPD2 –ö–û–î–û–í ===\")\n",
    "count_distribution = okpd2_counts.value_counts().sort_index()\n",
    "for count, freq in count_distribution.items():\n",
    "    print(f\"  {count} –∫–æ–¥(–æ–≤): {freq} –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ ({freq/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 3: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π\n",
    "df['full_text'] = df['contractSubject'].fillna('') + '. ' + df['contractSubjectFull'].fillna('')\n",
    "\n",
    "print(\"–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:\")\n",
    "print(df['full_text'].str.len().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return \"\"\n",
    "    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    text = text.lower()\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
    "    text = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø0-9\\s\\.]', ' ', text)\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    return text.strip()\n",
    "\n",
    "df['text_clean'] = df['full_text'].apply(clean_text)\n",
    "\n",
    "print(\"–ü—Ä–∏–º–µ—Ä –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞:\")\n",
    "print(\"–î–û:\", df['full_text'].iloc[0][:200])\n",
    "print(\"–ü–û–°–õ–ï:\", df['text_clean'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ)\n",
    "# !pip install pymorphy2 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "# –î–æ–º–µ–Ω–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "domain_stopwords = ['–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '–¥–æ–≥–æ–≤–æ—Ä', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–ø—É–Ω–∫—Ç', '—Å—Ç–∞—Ç—å—è', \n",
    "                   '–¥–∞–ª–µ–µ', '—Å–æ–≥–ª–∞—Å–Ω–æ', '—Ç–∞–∫–∂–µ', '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–∏–Ω–æ–π', '–¥—Ä—É–≥–æ–π',\n",
    "                   '–æ–±—è–∑–∞–Ω', '–æ–±—è–∑–∞–Ω–∞', '–æ–±—è–∑–∞–Ω—ã', '–æ–±—è–∑–∞–Ω–æ', '—É—Å–ª–æ–≤–∏–µ', '—Å–ª–µ–¥—É—é—â–∏–π']\n",
    "custom_stopwords = set(russian_stopwords + domain_stopwords)\n",
    "\n",
    "def advanced_text_processing(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "    tokens = [token for token in tokens if token not in custom_stopwords]\n",
    "    \n",
    "    # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    lemmatized_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            lemma = morph.parse(token)[0].normal_form\n",
    "            lemmatized_tokens.append(lemma)\n",
    "        except:\n",
    "            lemmatized_tokens.append(token)\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –¥–∞–Ω–Ω—ã–º\n",
    "print(\"–ù–∞—á–∞—Ç–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...\")\n",
    "df['text_processed'] = df['text_clean'].apply(advanced_text_processing)\n",
    "print(\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:\")\n",
    "print(\"–î–û:\", df['text_clean'].iloc[0][:150])\n",
    "print(\"–ü–û–°–õ–ï:\", df['text_processed'].iloc[0][:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 4: –†–ê–ó–ú–ï–¢–ö–ê –î–ê–ù–ù–´–•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== –ê–ù–ê–õ–ò–ó –°–£–©–ï–°–¢–í–£–Æ–©–ï–ô –†–ê–ó–ú–ï–¢–ö–ò ===\")\n",
    "print(f\"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}\")\n",
    "print(f\"–ó–∞–ø–∏—Å–µ–π —Å OKPD2: {df['OKPD2_codes'].notna().sum()}\")\n",
    "print(f\"–ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {df['OKPD2_codes'].notna().mean():.2%}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã OKPD2 –∫–æ–¥–æ–≤\n",
    "df['okpd_count'] = df['OKPD2_codes'].apply(lambda x: len(x) if x else 0)\n",
    "print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–¥–æ–≤ OKPD2 –Ω–∞ –∑–∞–ø–∏—Å—å:\")\n",
    "print(df['okpd_count'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç–æ–∫\n",
    "def extract_okpd_label(okpd_codes):\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ OKPD2 –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\"\"\"\n",
    "    if not okpd_codes or len(okpd_codes) == 0:\n",
    "        return None\n",
    "    \n",
    "    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∫–æ–¥ –∏–∑ —Å–ø–∏—Å–∫–∞\n",
    "    primary_code = okpd_codes[0]\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ (–Ω–∞—á–∏–Ω–∞–µ–º —Å 2-–∑–Ω–∞—á–Ω–æ–≥–æ –∫–æ–¥–∞)\n",
    "    if len(primary_code) >= 2:\n",
    "        return primary_code[:2]  # –ü–µ—Ä–≤—ã–µ 2 —Ü–∏—Ñ—Ä—ã - —Ä–∞–∑–¥–µ–ª\n",
    "    else:\n",
    "        return primary_code\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
    "df['target'] = df['OKPD2_codes'].apply(extract_okpd_label)\n",
    "\n",
    "print(\"–£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—Ç–æ–∫:\")\n",
    "print(df['target'].str.len().value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ç–∫–∏\n",
    "print(\"=== –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –†–ê–ó–ú–ï–¢–ö–ò ===\")\n",
    "\n",
    "# –°–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤\n",
    "sample_classes = df['target'].value_counts().head(3).index\n",
    "\n",
    "for class_label in sample_classes:\n",
    "    class_texts = df[df['target'] == class_label]['text_clean'].head(2)\n",
    "    print(f\"\\n--- –ö–ª–∞—Å—Å {class_label} ({len(df[df['target'] == class_label])} –ø—Ä–∏–º–µ—Ä–æ–≤) ---\")\n",
    "    for i, text in enumerate(class_texts):\n",
    "        print(f\"{i+1}. {text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–±–ª–µ–º —Ä–∞–∑–º–µ—Ç–∫–∏\n",
    "missing_target = df['target'].isna().sum()\n",
    "print(f\"–ó–∞–ø–∏—Å–µ–π –±–µ–∑ –º–µ—Ç–æ–∫: {missing_target}\")\n",
    "\n",
    "# –ê–Ω–∞–ª–∏–∑ –º–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤\n",
    "class_counts = df['target'].value_counts()\n",
    "small_classes = class_counts[class_counts < 5]\n",
    "print(f\"–ú–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (–º–µ–Ω–µ–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤): {len(small_classes)}\")\n",
    "\n",
    "# –†–µ—à–µ–Ω–∏–µ: —É–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ –º–µ—Ç–æ–∫ –∏ –º–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã\n",
    "df_labeled = df[df['target'].notna()].copy()\n",
    "df_labeled = df_labeled[~df_labeled['target'].isin(small_classes.index)]\n",
    "\n",
    "print(f\"–ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df_labeled)}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {df_labeled['target'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ç–∫–∏\n",
    "print(\"\\n=== –í–ê–õ–ò–î–ê–¶–ò–Ø –†–ê–ó–ú–ï–¢–ö–ò (—Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã) ===\")\n",
    "sample_indices = random.sample(range(len(df_labeled)), 5)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = df_labeled.iloc[idx]\n",
    "    print(f\"\\n–ú–µ—Ç–∫–∞: {row['target']}\")\n",
    "    print(f\"–¢–µ–∫—Å—Ç: {row['text_clean'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 5: –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# –¢–æ–ø-20 –∫–ª–∞—Å—Å–æ–≤\n",
    "top_classes = df_labeled['target'].value_counts().head(20)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=top_classes.values, y=top_classes.index)\n",
    "plt.title('–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –û–ö–ü–î2')\n",
    "plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–æ–≤\n",
    "class_sizes = df_labeled['target'].value_counts()\n",
    "sns.histplot(class_sizes, bins=30)\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Å–æ–≤')\n",
    "plt.xlabel('–ü—Ä–∏–º–µ—Ä–æ–≤ –≤ –∫–ª–∞—Å—Å–µ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º:\")\n",
    "print(f\"–í—Å–µ–≥–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_sizes)}\")\n",
    "print(f\"–ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Å–∞: {class_sizes.median()}\")\n",
    "print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {class_sizes.min()}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {class_sizes.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤\n",
    "df_labeled['text_length'] = df_labeled['text_processed'].str.split().str.len()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=df_labeled, x='text_length', bins=50)\n",
    "plt.title('–û–±—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤')\n",
    "plt.xlabel('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Ç–æ–ø-10 –∫–ª–∞—Å—Å–∞–º\n",
    "top_10_classes = class_sizes.head(10).index\n",
    "df_top_classes = df_labeled[df_labeled['target'].isin(top_10_classes)]\n",
    "sns.boxplot(data=df_top_classes, x='target', y='text_length')\n",
    "plt.title('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º (—Ç–æ–ø-10)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤:\")\n",
    "print(df_labeled['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 6: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "X = df_labeled['text_processed']\n",
    "y = df_labeled['target']\n",
    "\n",
    "# 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 ‚âà 0.15\n",
    ")\n",
    "\n",
    "print(\"=== –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê –í–´–ë–û–†–ö–ò ===\")\n",
    "print(f\"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –∑–∞–ø–∏—Å–µ–π ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_val)} –∑–∞–ø–∏—Å–µ–π ({len(X_val)/len(X):.1%})\")\n",
    "print(f\"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –∑–∞–ø–∏—Å–µ–π ({len(X_test)/len(X):.1%})\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–∞—Ö\n",
    "print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ –≤—ã–±–æ—Ä–∫–∞–º:\")\n",
    "for name, split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "    print(f\"{name}: {split.nunique()} –∫–ª–∞—Å—Å–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 7: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# TF-IDF —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),  # –£—á–∏—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –ø–∞—Ä—ã\n",
    "    min_df=2,           # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞\n",
    "    max_df=0.9,         # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞\n",
    "    stop_words=list(custom_stopwords)\n",
    ")\n",
    "\n",
    "# Bag-of-Words –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    stop_words=list(custom_stopwords)\n",
    ")\n",
    "\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è TF-IDF...\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Bag-of-Words\n",
    "print(\"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è Bag-of-Words...\")\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_val_bow = bow_vectorizer.transform(X_val)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\n–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–∞—Ç—Ä–∏—Ü –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
    "print(f\"TF-IDF: {X_train_tfidf.shape}\")\n",
    "print(f\"BOW: {X_train_bow.shape}\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ processed –¥–∞–Ω–Ω—ã—Ö\n",
    "df_processed = pd.DataFrame({\n",
    "    'text_processed': X,\n",
    "    'target': y\n",
    "})\n",
    "df_processed.to_csv('processed_contracts_data.csv', index=False)\n",
    "print(\"\\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'processed_contracts_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# –ß–µ–∫–ø–æ–π–Ω—Ç ‚Ññ3: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "\n",
    "# –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º\n",
    "key_metric = 'f1_macro'  # F1-score (macro average)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"–ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    print(f\"=== {model_name} ===\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 2: Baseline-–º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "print(\"=== BASELINE –ú–û–î–ï–õ–ò ===\")\n",
    "\n",
    "# KNN\n",
    "print(\"\\n–û–±—É—á–µ–Ω–∏–µ KNN...\")\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_tfidf, y_train)\n",
    "y_pred_knn = knn.predict(X_val_tfidf)\n",
    "knn_metrics = evaluate_model(y_val, y_pred_knn, \"KNN\")\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n–û–±—É—á–µ–Ω–∏–µ Logistic Regression...\")\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial')\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = lr.predict(X_val_tfidf)\n",
    "lr_metrics = evaluate_model(y_val, y_pred_lr, \"Logistic Regression\")\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"\\n–û–±—É—á–µ–Ω–∏–µ Naive Bayes...\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow, y_train)\n",
    "y_pred_nb = nb.predict(X_val_bow)\n",
    "nb_metrics = evaluate_model(y_val, y_pred_nb, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 3: –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"=== –ü–û–î–ë–û–† –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í ===\")\n",
    "\n",
    "# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'penalty': ['l2', 'none'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial'),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"–ó–∞–ø—É—Å–∫ GridSearch...\")\n",
    "lr_grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(f\"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {lr_grid.best_params_}\")\n",
    "print(f\"–õ—É—á—à–∏–π F1-score –Ω–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {lr_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 4: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "best_model = lr_grid.best_estimator_\n",
    "y_pred_best = best_model.predict(X_val_tfidf)\n",
    "best_metrics = evaluate_model(y_val, y_pred_best, \"Best Logistic Regression\")\n",
    "\n",
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π\n",
    "print(\"\\n=== –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===\")\n",
    "models_comparison = pd.DataFrame({\n",
    "    'KNN': knn_metrics,\n",
    "    'LogisticRegression': lr_metrics,\n",
    "    'NaiveBayes': nb_metrics,\n",
    "    'BestModel': best_metrics\n",
    "})\n",
    "\n",
    "models_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
    "models_comparison.T[metrics_to_plot].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(y_val, y_pred_best, labels=best_model.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45, ax=plt.gca())\n",
    "plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - Best Logistic Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫\n",
    "error_analysis = pd.DataFrame({\n",
    "    'true': y_val,\n",
    "    'predicted': y_pred_best,\n",
    "    'text': X_val\n",
    "})\n",
    "\n",
    "errors = error_analysis[y_val != y_pred_best]\n",
    "error_counts = errors.groupby(['true', 'predicted']).size().reset_index(name='count')\n",
    "error_counts = error_counts.sort_values('count', ascending=False)\n",
    "\n",
    "print(\"–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\")\n",
    "print(error_counts.head(10))\n",
    "\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫:\")\n",
    "for i in range(min(3, len(errors))):\n",
    "    row = errors.iloc[i]\n",
    "    print(f\"\\n–û—à–∏–±–∫–∞ {i+1}:\")\n",
    "    print(f\"–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {row['true']}\")\n",
    "    print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {row['predicted']}\")\n",
    "    print(f\"–¢–µ–∫—Å—Ç: {row['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∞–≥ 5: –§–∏–Ω–∞–ª—å–Ω—ã–µ –≤—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== –§–ò–ù–ê–õ–¨–ù–´–ï –í–´–í–û–î–´ ===\")\n",
    "\n",
    "print(f\"1. –†–ê–ó–ú–ï–¢–ö–ê –î–ê–ù–ù–´–•:\")\n",
    "print(f\"   - –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(df)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "print(f\"   - –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –∏ —Ä–∞–∑–º–µ—Ç–∫–∏: {len(df_labeled)} –∑–∞–ø–∏—Å–µ–π\")\n",
    "print(f\"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {df_labeled['target'].nunique()}\")\n",
    "print(f\"   - –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: –º–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Å–∞ {class_sizes.median()}\")\n",
    "\n",
    "print(f\"\\n2. –ö–ê–ß–ï–°–¢–í–û –ú–û–î–ï–õ–ï–ô (F1-macro):\")\n",
    "print(f\"   - KNN: {knn_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   - Naive Bayes: {nb_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   - Logistic Regression: {lr_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   - Best Model: {best_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\")\n",
    "print(f\"   - –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {type(best_model).__name__}\")\n",
    "print(f\"   - –ö–ª—é—á–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞: {key_metric}\")\n",
    "print(f\"   - –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ –∫–ª–∞—Å—Å—ã\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "import joblib\n",
    "\n",
    "model_artifacts = {\n",
    "    'model': best_model,\n",
    "    'vectorizer': tfidf_vectorizer,\n",
    "    'metrics': best_metrics\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifacts, 'best_model_artifacts.pkl')\n",
    "print(f\"\\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'best_model_artifacts.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
